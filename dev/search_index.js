var documenterSearchIndex = {"docs":
[{"location":"api/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"api/","page":"Reference","title":"Reference","text":"Modules = [ColBERT]","category":"page"},{"location":"api/#ColBERT.ColBERTConfig","page":"Reference","title":"ColBERT.ColBERTConfig","text":"ColBERTConfig(; use_gpu::Bool, rank::Int, nranks::Int, query_token_id::String,\n        doc_token_id::String, query_token::String, doc_token::String, checkpoint::String,\n        collection::String, dim::Int, doc_maxlen::Int, mask_punctuation::Bool,\n        query_maxlen::Int, attend_to_mask_tokens::Bool, index_path::String,\n        index_bsize::Int, nbits::Int, kmeans_niters::Int, nprobe::Int, ncandidates::Int)\n\nStructure containing config for running and training various components.\n\nArguments\n\nuse_gpu: Whether to use a GPU or not. Default is false.\nrank: The index of the running GPU. Default is 0. For now, the package only allows this to be 0.\nnranks: The number of GPUs used in the run. Default is 1. For now, the package only supports one GPU.\nquery_token_id: Unique identifier for query tokens (defaults to [unused0]).\ndoc_token_id: Unique identifier for document tokens (defaults to [unused1]).\nquery_token: Token used to represent a query token (defaults to [Q]).\ndoc_token: Token used to represent a document token (defaults to [D]).\ncheckpoint: The path to the HuggingFace checkpoint of the underlying ColBERT model. Defaults to \"colbert-ir/colbertv2.0\".\ncollection: Path to the file containing the documents. Default is \"\".\ndim: The dimension of the document embedding space. Default is 128.\ndoc_maxlen: The maximum length of a document before it is trimmed to fit. Default is 220.\nmask_punctuation: Whether or not to mask punctuation characters tokens in the document. Default is true.\nquery_maxlen: The maximum length of queries after which they are trimmed.\nattend_to_mask_tokens: Whether or not to attend to mask tokens in the query. Default value is false.\nindex_path: Path to save the index files.\nindex_bsize: Batch size used for some parts of indexing.\nchunksize: Custom size of a chunk, i.e the number of passages for which data is to be stored in one chunk. Default is missing, in which case chunksize is determined from the size of the collection and nranks.\npassages_batch_size: The number of passages sent as a batch to encoding functions. Default is 300.\nnbits: Number of bits used to compress residuals.\nkmeans_niters: Number of iterations used for k-means clustering.\nnprobe: The number of nearest centroids to fetch during a search. Default is 2. Also see retrieve.\nncandidates: The number of candidates to get during candidate generation in search. Default is 8192. Also see retrieve.\n\nReturns\n\nA ColBERTConfig object.\n\nExamples\n\nMost users will just want to use the defaults for most settings. Here's a minimal example:\n\njulia> using ColBERT;\n\njulia> config = ColBERTConfig(\n           use_gpu = true,\n           collection = \"/home/codetalker7/documents\",\n           index_path = \"./local_index\"\n       );\n\n\n\n\n\n","category":"type"},{"location":"api/#ColBERT.Indexer-Tuple{ColBERTConfig}","page":"Reference","title":"ColBERT.Indexer","text":"Indexer(config::ColBERTConfig)\n\nType representing an ColBERT indexer.\n\nArguments\n\nconfig: The ColBERTConfig used to build the index.\n\nReturns\n\nAn [Indexer] wrapping a ColBERTConfig along with the trained ColBERT model.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._add_marker_row-Union{Tuple{T}, Tuple{AbstractMatrix{T}, T}} where T","page":"Reference","title":"ColBERT._add_marker_row","text":"_add_marker_row(data::AbstractMatrix{T}, marker::T) where {T}\n\nAdd row containing marker as the second row of data.\n\nArguments\n\ndata: The matrix in which the row is to be added.\nmarker: The marker to be added.\n\nReturns\n\nA matrix equal to data, with the second row being filled with marker.\n\nExamples\n\njulia> using ColBERT: _add_marker_row;\n\njulia> x = ones(Float32, 5, 5);\n5×5 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n\njulia> _add_marker_row(x, zero(Float32))\n6×5 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  1.0\n 0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._binarize-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:Integer","page":"Reference","title":"ColBERT._binarize","text":"Examples\n\njulia> using ColBERT: _binarize;\n\njulia> using Flux, CUDA, Random;\n\njulia> Random.seed!(0);\n\njulia> nbits = 5;\n\njulia> data = rand(0:2^nbits - 1, 100, 200000) |> Flux.gpu\n100×200000 CuArray{Int64, 2, CUDA.DeviceMemory}:\n 12  23  11   6   5   2  27   1   0   4  15   8  24  …   4  25  22  18   4   0  15  16   3  25   4  13\n  2  11  29   8  31   3  15   1   8   1  22  22  10     25  25   1  12  21  13  27  20  23  24   9  14\n 27   4   4  15   4   9  19   4   3  10  27  14   3     10   8  18  19  12   9  29  23   8  15  30  21\n  2   7   4   5  25  16  27  23   5  24  26  19   9     22   1  21  12  31  20   4  31  26  21  25   6\n 21  18  25   9   9  17   6  20  16  13  14   2   2     28  13  11   9  22   4   2  22  27  24   9  31\n  3  26  22   8  24  23  29  19  13   3   2  20  14  …  22  18  18   5  16   5   9   3  21  19  17  23\n  3  13   5   9   8  12  24  26   8  10  14   1  21     14  25  18   5   1   4  13   0  14  11  16   8\n 22  20  22   6  25   1  29  23   9  21  13  27   6     11  21   4  31  14  14   5  27  17   6  27  19\n  9   2   7   2  16   1  23  15   2  17  30  18   4     26   5  20  31  18   8  20  13  23  26  29  25\n  0   6  20   8   0  18   9  28   8  30   6   2  21      0   7  25  23  19   2   6  27  13   3   6  22\n 17   2   0  13  26   6   7   8  14  20  11   9  17  …  29   4  28  22   1  10  29  20  11  20  30   8\n 28   5   0  30   1  26  23   9  29   9  29   2  15     27   8  13  11  27   6  11   7  19   4   7  28\n  8   9  16  29  22   8   9  19  30  20   4   0   1      1  25  14  16  17  26  28  31  25   4  22  23\n 10   9  31  22  20  15   1   9  26   2   0   1  27     23  21  15  22  29  29   1  24  30  22  17  22\n 13   8  23   9   1   6   2  28  18   1  15   5  12     28  27   3   6  22   3  20  24   3   2   2  29\n 28  22  19   7  20  28  25  13   3  13  17  31  28  …  18  17  19   6  20  11  31   9  28   9  19   1\n 23   1   7  14   6  14   0   9   1   9  12  30  24     23   2  13   9   0  20  17   4  16  22  27  11\n  4  19   8  31  14  30   2  13  27  16  29  10  30     29  25  28  31  13  11   8  12  30  13  10   7\n 18  26  30   6  31   6  15  11  10  31  21  24  11     19  19  29  17  13   5   3  28  29  31  22  13\n 14  29  18  14  25  10  28  28  15   8   5  14   5     10  17  13  23   0  26  25  13  15  26   3   5\n  0   4  24  23  20  16  25   9  17  27  15   0  10  …   5  18   2   2  30  17   8  11  27  11  15  27\n 15   2  22   8   6   8  16   2   8  24  26  15  30     27  12  28  31  26  18   4  10   5  16  23  16\n 20  20  29  24   1   9  18  31  16   3   9  17  31      8   4   4  15  13  16   0  10  31  28   8  29\n  2   3   2  23  15  21   6   8  21   7  17  15  17      7  15  19  25   3   2  11  26  16  12  11  27\n 13  21  22  20  15   0  22   2  30  14  14  20  26     13  23  14  18   0  24  21  17   8  11  26  22\n  ⋮                   ⋮                   ⋮          ⋱           ⋮                   ⋮\n  9   7   1   1  28  28  10  16  23  18  26   9   7  …  14   5  12   3   6  25  20   5  13   3  20  10\n 28  25  21   8  31   4  25   7  27  26  19   4   9     15  26   2  23  14  16  29  17  11  29  12  18\n  4  15  20   2   3  10   6   9  13  22   5  28  21     12  11  12  14  14   9  13  31  12   6   9  21\n  9  24   2   4  27  14   4  15  19   2  14  30   3     17   5   6   2  23  15  11   1   0  10   0  28\n 20   0  26   8  21   7   1   7  22  10  10   5  31     23   5  20  11  29  12  25  14  13   5  25  15\n  2   9  27  28  25   7  27  30  20   5  10   2  28  …  21  19  22  30  24   0  10  19  10  30  22   9\n 10   2  31  10  12  13  16  10   5  28  16   4  16      3   1  31  20  19  16  19  30  31  14   5  20\n 14   2  20  19  16  25   4   1  15  31  22  17   8     12  19   9  29  30  20  13  19  14  18   7  22\n 20   3  27  23   9  21  20  10  14   3   5  26  22     19  19  11   3  22  19  24  12  27  12  28  17\n  1  27  27  10   8  29  17  14  19   6   6  12   6     10   6  24  29  26  11   2  25   7   6   1  28\n 11  19   5   1   7  19   8  17  27   4   4   7   0  …  13  29   0  15  15   2   2   6  24   0   5  18\n 17  31  31  23  24  18   0  31   6  22  20  31  23     16   5   8  17   6  20  23  21  26  15  27  30\n  1   6  30  31   8   3  28  31  10  23  23  24  26     12  30  10   3  25  24  12  20   8   7  14  11\n 26  22  23  21  24   7   2  19  10  27  21  14   7      7  27   1  29   7  23  30  24  12   9  12  14\n 28  26   8  28  10  18  23  28  10  19  31  26  17     18  20  23   8  31  15  18  10  24  28   7  23\n  1   7  15  22  23   0  21  19  28  10  15  13   7  …  21  15  16   1  16   9  25  23   1  24  20   5\n 21   7  30  30   5   0  27  26   6   7  30   2  16      2  16   6   9   6   4  12   4  12  18  28  17\n 11  16   0  20  20  13  18  19  21   7  24   4  26      1  26   7  16   0   2   3   2  22  27  25  15\n  9  20  31  24  14  29  28  26  29  31   7  28  12     28   0  12   3  17   7   0  30  25  22  23  20\n 19  21  30  16  15  20  31   2   2   8  27  20  29     27  13   2  27   8  17  19  15   9  22   3  27\n 13  17   6   4   9   1  18   2  21  27  13  14  12  …  28  21   4   2  11  13  31  13  25  25  29  21\n  2  17  19  15  17   1  12   0  11   9  16   1  13     25  21  28  22   7  13   3  20   7   6  26  21\n 13   6   5  11  12   2   2   3   4  16  30  14  19     16   5   5  19  17   3  31  26  19   2  11  15\n 20  30  21  30  13  26   7   9  11  18   3   0  15      3  14  15   1   9  16   1  16   0   2   2  11\n  3  24   6  16  10   3   7  17   0  30   9  14   1     29   4   8   4  17  14  27   0  17  12   4  19\n\njulia> _binarize(data, nbits)\n5×100×200000 CuArray{Bool, 3, CUDA.DeviceMemory}:\n[:, :, 1] =\n 0  0  1  0  1  1  1  0  1  0  1  0  0  0  1  0  1  0  0  …  0  0  0  1  1  1  1  0  0  1  1  1  1  1  1  0  1  0  1\n 0  1  1  1  0  1  1  1  0  0  0  0  0  1  0  0  1  0  1     1  1  0  0  1  0  0  1  0  0  0  1  0  1  0  1  0  0  1\n 1  0  0  0  1  0  0  1  0  0  0  1  0  0  1  1  1  1  0     0  1  1  0  0  0  0  0  1  0  1  0  0  0  1  0  1  1  0\n 1  0  1  0  0  0  0  0  1  0  0  1  1  1  1  1  0  0  0     1  1  0  0  1  0  0  1  1  0  0  1  1  0  1  0  1  0  0\n 0  0  1  0  1  0  0  1  0  0  1  1  0  0  0  1  1  0  1     0  0  1  0  0  1  0  1  1  0  1  0  0  1  0  0  0  1  0\n\n[:, :, 2] =\n 1  1  0  1  0  0  1  0  0  0  0  1  1  1  0  0  1  1  0  …  0  0  1  1  1  1  0  0  0  1  1  0  0  1  1  1  0  0  0\n 1  1  0  1  1  1  0  0  1  1  1  0  0  0  0  1  0  1  1     1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  1  1  0\n 1  0  1  1  0  0  1  1  0  1  0  1  0  0  0  1  0  0  0     0  0  0  0  0  1  1  1  0  1  1  0  1  1  0  0  1  1  0\n 0  1  0  0  0  1  1  0  0  0  0  0  1  1  1  0  0  0  1     0  0  0  1  0  1  0  0  1  0  0  0  0  0  0  0  0  1  1\n 1  0  0  0  1  1  0  1  0  0  0  0  0  0  0  1  0  1  1     0  0  0  1  1  1  0  1  1  0  0  1  1  1  1  1  0  1  1\n\n[:, :, 3] =\n 1  1  0  0  1  0  1  0  1  0  0  0  0  1  1  1  1  0  0  …  1  0  1  1  1  1  0  1  0  1  0  0  1  0  0  1  1  1  0\n 1  0  0  0  0  1  0  1  1  0  0  0  0  1  1  1  1  0  1     1  0  1  1  0  1  1  1  0  1  1  0  1  1  1  1  0  0  1\n 0  1  1  1  0  1  1  1  1  1  0  0  0  1  1  0  1  0  1     1  1  0  0  1  1  1  1  0  1  1  0  1  1  1  0  1  1  1\n 1  1  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0  1  1     1  0  1  1  0  1  1  0  1  1  1  0  1  1  0  0  0  0  0\n 0  1  0  0  1  1  0  1  0  1  0  0  1  1  1  1  0  0  1     1  1  1  1  0  1  1  1  0  0  1  0  1  1  0  1  0  1  0\n\n;;; …\n\n[:, :, 199998] =\n 1  0  1  1  0  1  1  0  0  1  0  0  0  0  0  1  0  1  1  …  0  0  0  0  0  1  1  1  0  0  0  1  0  0  1  0  0  0  0\n 0  0  1  0  0  1  1  1  1  1  0  0  0  1  1  0  1  0  1     1  1  0  1  0  1  1  0  0  0  1  1  1  1  0  1  1  1  0\n 0  0  1  1  0  0  0  1  0  0  1  1  1  1  0  0  1  1  1     1  0  1  1  0  1  1  0  1  0  0  0  1  1  0  1  0  0  1\n 1  1  1  0  1  0  1  0  1  0  0  0  0  0  0  1  0  1  1     1  0  1  0  0  1  0  1  1  1  0  1  0  0  1  0  0  0  1\n 1  1  0  1  1  1  0  0  1  0  1  0  0  1  0  0  1  0  1     0  1  0  0  0  0  0  0  1  1  1  1  1  1  1  0  0  0  0\n\n[:, :, 199999] =\n 0  1  0  1  1  1  0  1  1  0  0  1  0  1  0  1  1  0  0  …  1  1  0  1  1  1  0  0  1  0  0  1  1  1  1  0  1  0  0\n 0  0  1  0  0  0  0  1  0  1  1  1  1  0  1  1  1  1  1     0  1  0  0  0  1  1  0  1  0  0  0  1  1  0  1  1  1  0\n 1  0  1  0  0  0  0  0  1  1  1  1  1  0  0  0  0  0  1     1  1  1  0  1  0  1  1  1  1  1  0  1  0  1  0  0  0  1\n 0  1  1  1  1  0  0  1  1  0  1  0  0  0  0  0  1  1  0     0  0  1  0  0  1  1  1  0  0  1  1  0  0  1  1  1  0  0\n 0  0  1  1  0  1  1  1  1  0  1  0  1  1  0  1  1  0  1     0  0  1  0  0  1  0  0  0  1  1  1  1  0  1  1  0  0  0\n\n[:, :, 200000] =\n 1  0  1  0  1  1  0  1  1  0  0  0  1  0  1  1  1  1  1  …  0  0  1  0  0  0  1  0  1  1  1  1  0  1  1  1  1  1  1\n 0  1  0  1  1  1  0  1  0  1  0  0  1  1  0  0  1  1  0     0  1  0  0  1  1  1  1  1  0  0  1  0  1  0  0  1  1  1\n 1  1  1  1  1  1  0  0  0  1  0  1  1  1  1  0  0  1  1     1  1  0  1  0  1  0  1  1  1  0  1  1  0  1  1  1  0  0\n 1  1  0  0  1  0  1  0  1  0  1  1  0  0  1  0  1  0  1     0  0  0  1  0  1  1  1  0  0  0  1  0  1  0  0  1  1  0\n 0  0  1  0  1  1  0  1  1  1  0  1  1  1  1  0  0  0  0     1  1  1  1  1  1  0  0  1  0  1  0  1  1  1  1  0  0  1\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._bucket_indices-Union{Tuple{S}, Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{S}}} where {T<:Number, S<:Number}","page":"Reference","title":"ColBERT._bucket_indices","text":"Examples\n\njulia> using ColBERT: _bucket_indices;\n\njulia> using Random; Random.seed!(0);\n\njulia> data = rand(50, 50) |> Flux.gpu;\n50×50 CuArray{Float32, 2, CUDA.DeviceMemory}:\n 0.455238   0.828104   0.735106   0.042069   …  0.916387    0.10078      0.00907127\n 0.547642   0.100748   0.993553   0.0275458     0.0954245   0.351846     0.548682\n 0.773354   0.908416   0.703694   0.839846      0.613082    0.605597     0.660227\n 0.940585   0.932748   0.150822   0.920883      0.754362    0.843869     0.0453409\n 0.0296477  0.123079   0.409406   0.672372      0.19912     0.106127     0.945276\n 0.746943   0.149248   0.864755   0.116243   …  0.541295    0.224275     0.660706\n 0.746801   0.743713   0.64608    0.446445      0.951642    0.583662     0.338174\n 0.97667    0.722362   0.692789   0.646206      0.089323    0.305554     0.454803\n 0.329335   0.785124   0.254097   0.271299      0.320879    0.000438984  0.161356\n 0.672001   0.532197   0.869579   0.182068      0.289906    0.068645     0.142121\n 0.0997382  0.523732   0.315933   0.935547   …  0.819027    0.770597     0.654065\n 0.230139   0.997278   0.455917   0.566976      0.0180972   0.275211     0.0619634\n 0.631256   0.709048   0.810256   0.754144      0.452911    0.358555     0.116042\n 0.096652   0.454081   0.715283   0.923417      0.498907    0.781054     0.841858\n 0.69801    0.0439444  0.27613    0.617714      0.589872    0.708365     0.0266968\n 0.470257   0.654557   0.351769   0.812597   …  0.323819    0.621386     0.63478\n 0.114864   0.897316   0.0243141  0.910847      0.232374    0.861399     0.844008\n 0.984812   0.491806   0.356395   0.501248      0.651833    0.173494     0.38356\n 0.730758   0.970359   0.456407   0.8044        0.0385577   0.306404     0.705577\n 0.117333   0.233628   0.332989   0.0857914     0.224095    0.747571     0.387572\n ⋮                                           ⋱\n 0.908402   0.609104   0.108874   0.430905   …  0.00564743  0.964602     0.541285\n 0.570179   0.10114    0.210174   0.945569      0.149051    0.785343     0.241959\n 0.408136   0.221389   0.425872   0.204654      0.238413    0.583185     0.271998\n 0.526989   0.0401535  0.686314   0.534208      0.29416     0.488244     0.747676\n 0.129952   0.716592   0.352166   0.584363      0.0850619   0.161153     0.243575\n 0.0256413  0.0831649  0.179467   0.799997   …  0.229072    0.711857     0.326977\n 0.939913   0.21433    0.223666   0.914527      0.425202    0.129862     0.766065\n 0.600877   0.516631   0.753827   0.674017      0.665329    0.622929     0.645962\n 0.223773   0.257933   0.854171   0.259882      0.298119    0.231662     0.824881\n 0.268817   0.468576   0.218589   0.835418      0.802857    0.0159643    0.0330232\n 0.408092   0.361884   0.849442   0.527004   …  0.0500168   0.427498     0.70482\n 0.740789   0.952265   0.722908   0.0856596     0.507305    0.32629      0.117663\n 0.873501   0.587707   0.894573   0.355338      0.345011    0.0693833    0.457268\n 0.758824   0.162728   0.608327   0.902837      0.492069    0.716635     0.459272\n 0.922832   0.950539   0.51935    0.52672       0.725665    0.36443      0.936056\n 0.239929   0.3754     0.247219   0.92438    …  0.0763809   0.737196     0.712317\n 0.76676    0.182714   0.866055   0.749239      0.132254    0.755823     0.0869469\n 0.378313   0.0392607  0.93354    0.908511      0.733769    0.552135     0.351491\n 0.811121   0.891591   0.610976   0.0427439     0.0258436   0.482621     0.193291\n 0.109315   0.474986   0.140528   0.776382      0.609791    0.49946      0.116989\n\njulia> bucket_cutoffs = sort(rand(5)) |> Flux.gpu;\n5-element CuArray{Float32, 1, CUDA.DeviceMemory}:\n 0.42291805\n 0.7075339\n 0.8812783\n 0.89976573\n 0.9318977\n\njulia> _bucket_indices(data, bucket_cutoffs)\n50×50 CuArray{Int64, 2, CUDA.DeviceMemory}:\n 1  2  2  0  1  0  2  0  0  2  0  1  1  0  …  0  0  0  1  1  0  2  2  4  0  4  0  0\n 1  0  5  0  1  4  1  2  0  0  5  1  0  0     0  0  1  2  4  2  0  0  0  2  0  0  1\n 2  4  1  2  1  0  5  0  1  1  0  0  0  1     2  5  1  1  1  1  1  1  0  5  1  1  1\n 5  5  0  4  0  0  1  2  4  0  4  1  0  0     5  5  4  2  1  0  2  0  1  0  2  2  0\n 0  0  0  1  0  0  1  1  0  2  0  1  2  0     1  0  2  0  2  0  2  1  1  5  0  0  5\n 2  0  2  0  1  0  1  0  2  4  2  2  0  2  …  0  1  0  4  0  5  0  0  0  2  1  0  1\n 2  2  1  1  1  0  3  0  2  0  1  1  5  0     2  0  0  0  0  1  0  5  5  1  5  1  0\n 5  2  1  1  2  5  0  0  1  3  0  1  0  1     0  0  0  0  0  1  4  0  1  0  0  0  1\n 0  2  0  0  1  1  0  5  2  0  2  2  2  2     0  0  5  5  0  0  2  2  0  2  0  0  0\n 1  1  2  0  2  4  5  5  1  0  2  2  2  0     0  0  1  1  1  0  0  1  1  2  0  0  0\n 0  1  0  5  0  0  2  0  2  0  0  3  0  0  …  1  2  0  5  0  1  2  0  0  0  2  2  1\n 0  5  1  1  2  1  0  1  1  0  0  1  1  0     5  0  0  2  2  0  3  1  1  4  0  0  0\n 1  2  2  2  2  1  1  5  0  0  0  1  0  5     0  1  1  0  0  0  2  0  2  0  1  0  0\n 0  1  2  4  1  2  1  2  0  2  2  0  0  0     0  1  0  1  0  1  3  1  1  1  1  2  2\n 1  0  0  1  4  0  2  2  5  4  0  3  0  1     3  0  0  0  0  5  0  1  2  0  1  2  0\n 1  1  0  2  0  1  5  3  1  2  5  2  1  2  …  1  1  2  0  0  0  2  1  2  3  0  1  1\n 0  3  0  4  0  0  0  0  0  0  0  0  0  1     1  1  1  2  0  1  0  2  3  0  0  2  2\n 5  1  0  1  2  0  2  0  0  2  0  0  1  0     1  4  0  2  0  0  0  0  1  0  1  0  0\n 2  5  1  2  0  1  0  2  5  1  1  1  5  0     1  1  0  0  2  0  1  0  4  0  0  0  1\n 0  0  0  0  0  2  3  1  0  1  1  0  1  2     0  1  1  1  1  0  0  0  5  1  0  2  0\n ⋮              ⋮              ⋮           ⋱           ⋮              ⋮\n 4  1  0  1  4  1  2  0  1  0  0  1  0  2  …  0  0  0  0  0  2  0  2  0  1  0  5  1\n 1  0  0  5  2  2  5  0  0  3  5  0  1  5     1  2  0  1  2  0  0  0  1  0  0  2  0\n 0  0  1  0  0  1  4  0  0  1  0  5  1  5     1  1  2  0  2  0  1  1  2  4  0  1  0\n 1  0  1  1  0  0  0  0  1  0  0  0  0  4     0  0  1  0  3  5  0  1  1  1  0  1  2\n 0  2  0  1  0  0  2  0  2  1  1  2  1  1     0  0  0  1  1  1  0  0  1  2  0  0  0\n 0  0  0  2  5  2  2  0  0  5  5  4  1  0  …  0  0  2  1  5  0  1  0  1  0  0  2  0\n 5  0  0  4  0  1  0  0  0  1  2  2  0  0     1  0  0  0  1  1  4  0  5  1  1  0  2\n 1  1  2  1  1  1  0  0  0  0  0  2  1  0     0  5  0  1  0  0  1  2  0  0  1  1  1\n 0  0  2  0  0  1  1  4  0  2  2  0  5  1     1  1  1  1  5  0  3  2  2  1  0  0  2\n 0  1  0  2  2  1  1  0  1  0  1  0  0  2     5  0  1  0  5  0  0  2  2  0  2  0  0\n 0  0  2  1  0  1  1  1  1  2  4  0  1  2  …  1  1  1  1  0  0  5  1  0  0  0  1  1\n 2  5  2  0  0  0  2  0  2  0  0  0  0  0     4  0  5  5  0  2  0  0  0  0  1  0  0\n 2  1  3  0  1  1  0  0  4  0  0  1  1  0     1  1  0  4  1  1  0  2  0  3  0  0  1\n 2  0  1  4  1  0  0  1  0  2  1  0  0  0     5  1  0  0  1  1  0  0  2  0  1  2  1\n 4  5  1  1  1  1  0  0  0  1  1  0  5  2     5  0  2  2  1  1  1  5  2  1  2  0  5\n 0  0  0  4  2  1  0  3  0  3  2  0  1  2  …  0  1  0  2  0  0  2  5  2  0  0  2  2\n 2  0  2  2  1  0  0  3  1  1  0  5  2  0     2  0  2  0  5  1  0  0  1  0  0  2  0\n 0  0  5  4  1  0  2  2  2  0  1  1  2  5     0  0  0  0  1  0  0  1  0  1  2  1  0\n 2  3  1  0  0  2  0  0  5  0  5  0  1  1     0  0  5  2  0  1  0  5  2  1  0  1  0\n 0  1  0  2  1  0  2  2  1  0  1  4  1  1     5  1  0  1  4  1  1  1  1  1  1  1  0\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._cids_to_eids!-NTuple{4, Vector{Int64}}","page":"Reference","title":"ColBERT._cids_to_eids!","text":"_cids_to_eids!(eids::Vector{Int}, centroid_ids::Vector{Int},\n    ivf::Vector{Int}, ivf_lengths::Vector{Int})\n\nGet the set of embedding IDs contained in centroid_ids.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._compute_avg_residuals!-Tuple{Int64, AbstractMatrix{Float32}, AbstractMatrix{Float32}, AbstractVector{UInt32}}","page":"Reference","title":"ColBERT._compute_avg_residuals!","text":"_compute_avg_residuals!(\n    nbits::Int, centroids::AbstractMatrix{Float32},\n    heldout::AbstractMatrix{Float32}, codes::AbstractVector{UInt32})\n\nCompute the average residuals and other statistics of the held-out sample embeddings.\n\nArguments\n\nnbits: The number of bits used to compress the residuals.\ncentroids: A matrix containing the centroids of the computed using a k-means clustering algorithm on the sampled embeddings. Has shape (D, indexer.num_partitions), where D is the embedding dimension (128) and indexer.num_partitions is the number of clusters.\nheldout: A matrix containing the held-out embeddings, computed using _heldout_split.\ncodes: The array used to store the codes for each heldout embedding.\n\nReturns\n\nA tuple bucket_cutoffs, bucket_weights, avg_residual, which will be used in compression/decompression of residuals.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._integer_ids_and_mask-Tuple{Transformers.TextEncoders.AbstractTransformerTextEncoder, AbstractVector{String}}","page":"Reference","title":"ColBERT._integer_ids_and_mask","text":"_integer_ids_and_mask(\n    tokenizer::TextEncoders.AbstractTransformerTextEncoder,\n    batch_text::AbstractVector{String})\n\nRun batch_text through tokenizer to get matrices of tokens and attention mask.\n\nArguments\n\ntokenizer: The tokenizer to be used to tokenize the texts.\nbatch_text: The list of texts to tokenize.\n\nReturns\n\nA tuple integer_ids, bitmask, where integer_ids is a Matrix containing token IDs and bitmask is the attention mask.\n\nExamples\n\njulia> using ColBERT: _integer_ids_and_mask, load_hgf_pretrained_local;\n\njulia> tokenizer = load_hgf_pretrained_local(\"/home/codetalker7/models/colbertv2.0/:tokenizer\");\n\njulia> batch_text = [\n    \"hello world\",\n    \"thank you!\",\n    \"a\",\n    \"this is some longer text, so length should be longer\",\n    \"this is an even longer document. this is some longer text, so length should be longer\",\n];\n\njulia> integer_ids, bitmask = _integer_ids_and_mask(tokenizer, batch_text);\n\njulia> integer_ids\n20×5 Matrix{Int32}:\n  102   102   102   102   102\n 7593  4068  1038  2024  2024\n 2089  2018   103  2004  2004\n  103  1000     1  2071  2020\n    1   103     1  2937  2131\n    1     1     1  3794  2937\n    1     1     1  1011  6255\n    1     1     1  2062  1013\n    1     1     1  3092  2024\n    1     1     1  2324  2004\n    1     1     1  2023  2071\n    1     1     1  2937  2937\n    1     1     1   103  3794\n    1     1     1     1  1011\n    1     1     1     1  2062\n    1     1     1     1  3092\n    1     1     1     1  2324\n    1     1     1     1  2023\n    1     1     1     1  2937\n    1     1     1     1   103\n\njulia> bitmask\n20×5 BitMatrix:\n 1  1  1  1  1\n 1  1  1  1  1\n 1  1  1  1  1\n 1  1  0  1  1\n 0  1  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  0  1\n 0  0  0  0  1\n 0  0  0  0  1\n 0  0  0  0  1\n 0  0  0  0  1\n 0  0  0  0  1\n 0  0  0  0  1\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._load_model-Tuple{Transformers.HuggingFace.HGFConfig}","page":"Reference","title":"ColBERT._load_model","text":"_load_model(cfg::HF.HGFConfig; path_model::AbstractString,\n    trainmode::Bool = false, lazy::Bool = false, mmap::Bool = true)\n\nLocal model loader.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._load_tokenizer-Tuple{Transformers.HuggingFace.HGFConfig}","page":"Reference","title":"ColBERT._load_tokenizer","text":"_load_tokenizer(cfg::HF.HGFConfig; path_tokenizer_config::AbstractString,\n    path_special_tokens_map::AbstractString, path_tokenizer::AbstractString)\n\nLocal tokenizer loader.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._load_tokenizer_config-Tuple{AbstractString}","page":"Reference","title":"ColBERT._load_tokenizer_config","text":"_load_tokenizer_config(path_config)\n\nLoad tokenizer config locally.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._packbits-Tuple{AbstractArray{Bool, 3}}","page":"Reference","title":"ColBERT._packbits","text":"Examples\n\njulia> using ColBERT: _packbits;\n\njulia> using Random; Random.seed!(0);\n\njulia> bitsarray = rand(Bool, 2, 128, 200000);\n\njulia> _packbits(bitsarray)\n32×200000 Matrix{UInt8}:\n 0x2e  0x93  0x5a  0xbd  0xd1  0x89  0x2c  0x39  0x6a  …  0xed  0xdb  0x45  0x95  0xf8  0x64  0x57  0x5b  0x06\n 0x3f  0x45  0x0c  0x2a  0x14  0xdb  0x16  0x2b  0x00     0x70  0xba  0x3c  0x40  0x56  0xa6  0xbe  0x33  0x3d\n 0xbd  0x61  0xa3  0xa7  0xb4  0xe7  0x1e  0xf8  0xa7     0xf0  0x70  0xaf  0xc0  0xeb  0xa3  0x34  0x6d  0x81\n 0x15  0x9d  0x02  0xa5  0x7b  0x84  0xde  0x2f  0x28     0xa7  0xf2  0x51  0xb3  0xe7  0x01  0xbf  0x6f  0x5a\n 0xaf  0x76  0x8f  0x55  0x81  0x2f  0xa5  0xcc  0x03     0xe7  0xea  0x17  0xf2  0x07  0x45  0x40  0x40  0xd8\n 0xd2  0xd4  0x25  0xcc  0x41  0xc6  0x87  0x7e  0xfd  …  0x5a  0xe6  0xed  0x28  0x26  0x8b  0x39  0x3b  0x4b\n 0xb3  0xbe  0x08  0xdb  0x73  0x3d  0x58  0x04  0xda     0x7b  0xf7  0xab  0x1f  0x2d  0x7b  0x71  0x12  0xdf\n 0x6f  0x86  0x20  0x90  0xa5  0x0f  0xc7  0xeb  0x79     0x19  0x92  0x74  0x59  0x4b  0xfe  0xe2  0xb9  0xef\n 0x4b  0x93  0x7c  0x02  0x4f  0x40  0xad  0xe3  0x4f     0x9c  0x9c  0x69  0xd1  0xf8  0xd9  0x9e  0x00  0x70\n 0x77  0x5d  0x05  0xa6  0x2c  0xaa  0x9d  0xf6  0x8d     0xa9  0x4e  0x46  0x70  0xd9  0x47  0x80  0x06  0x7e\n 0x6e  0x7e  0x0f  0x3c  0xe7  0xaf  0x12  0xbf  0x0a  …  0x3f  0xaf  0xe8  0x57  0x26  0x4b  0x2c  0x3f  0x01\n 0x72  0xb1  0xea  0xde  0x97  0x1d  0xf4  0x4c  0x89     0x47  0x98  0xc5  0xb6  0x47  0xaf  0x95  0xb1  0x74\n 0xc6  0x2b  0x51  0x95  0x30  0xab  0xdc  0x29  0x79     0x5c  0x7b  0xc3  0xf4  0x6a  0xa6  0x09  0x39  0x96\n 0xeb  0xef  0x6f  0x70  0x8d  0x1f  0xb9  0x95  0x4e     0xd0  0xf5  0x68  0x0a  0x04  0x63  0x5b  0x45  0xf5\n 0xef  0xca  0xb7  0xd4  0x31  0x14  0x34  0x96  0x0c     0x1e  0x6a  0xce  0xf2  0xa3  0xa0  0xbe  0x92  0x9c\n 0xda  0x91  0x53  0xd1  0x43  0xfa  0x59  0x7a  0x0c  …  0x0f  0x7a  0xa0  0x4a  0x19  0xc6  0xd3  0xbb  0x7a\n 0x9a  0x81  0xdb  0xee  0xce  0x7e  0x4a  0xb5  0x2a     0x3c  0x3e  0xaa  0xdc  0xa6  0xd5  0xae  0x23  0xb2\n 0x82  0x2b  0xab  0x06  0xfd  0x8a  0x4a  0xba  0x80     0xb6  0x1a  0x62  0xa0  0x29  0x97  0x61  0x6e  0xf7\n 0xb8  0xe6  0x0d  0x21  0x38  0x3a  0x97  0x55  0x58     0x46  0x01  0xe1  0x82  0x34  0xa3  0xfa  0x54  0xb3\n 0x09  0xc7  0x2f  0x7b  0x82  0x0c  0x26  0x4d  0xa4     0x1e  0x64  0xc2  0x55  0x41  0x6b  0x14  0x5c  0x0b\n 0xf1  0x2c  0x3c  0x0a  0xf1  0x76  0xd4  0x57  0x42  …  0x44  0xb1  0xac  0xb4  0xa2  0x40  0x1e  0xbb  0x44\n 0xf8  0x0d  0x6d  0x09  0xb0  0x80  0xe3  0x5e  0x18     0xb3  0x43  0x22  0x82  0x0e  0x50  0xfb  0xf6  0x7b\n 0xf0  0x32  0x02  0x28  0x36  0x00  0x4f  0x84  0x2b     0xe8  0xcc  0x89  0x07  0x2f  0xf4  0xcb  0x41  0x53\n 0x53  0x9b  0x01  0xf3  0xb2  0x13  0x6a  0x43  0x88     0x22  0xd8  0x33  0xa2  0xab  0xaf  0xe1  0x02  0xf7\n 0x59  0x60  0x4a  0x1a  0x9c  0x29  0xb1  0x1b  0xea     0xe9  0xd6  0x07  0x78  0xc6  0xdf  0x16  0xff  0x87\n 0xba  0x98  0xff  0x98  0xc3  0xa3  0x7d  0x7c  0x75  …  0xfe  0x75  0x4d  0x43  0x8e  0x5e  0x32  0xb0  0x97\n 0x7b  0xc9  0xcf  0x4c  0x99  0xad  0xf1  0x0e  0x0d     0x9f  0xf2  0x92  0x75  0x86  0xd6  0x08  0x74  0x8d\n 0x7c  0xd4  0xe7  0x53  0xd3  0x23  0x25  0xce  0x3a     0x19  0xdb  0x14  0xa2  0xf1  0x01  0xd4  0x27  0x20\n 0x2a  0x63  0x51  0xcd  0xab  0xc3  0xb5  0xc1  0x74     0xa5  0xa4  0xe1  0xfa  0x13  0xab  0x1f  0x8f  0x9a\n 0x93  0xbe  0xf4  0x54  0x2b  0xb9  0x41  0x9d  0xa8     0xbf  0xb7  0x2b  0x1c  0x09  0x36  0xa5  0x7b  0xdc\n 0xdc  0x93  0x23  0xf8  0x90  0xaf  0xfb  0xd1  0xcc  …  0x54  0x09  0x8c  0x14  0xfe  0xa7  0x5d  0xd7  0x6d\n 0xaf  0x93  0xa2  0x29  0xf9  0x5b  0x24  0xd5  0x2a     0xf1  0x7f  0x3a  0xf5  0x8f  0xd4  0x6e  0x67  0x5b\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._sample_embeddings-Tuple{Transformers.HuggingFace.HGFBertModel, Transformers.Layers.Dense, Transformers.TextEncoders.AbstractTransformerTextEncoder, Int64, Int64, String, Vector{Int64}, Vector{String}}","page":"Reference","title":"ColBERT._sample_embeddings","text":"_sample_embeddings(bert::HF.HGFBertModel, linear::Layers.Dense,\n    tokenizer::TextEncoders.AbstractTransformerTextEncoder,\n    dim::Int, index_bsize::Int, doc_token::String,\n    skiplist::Vector{Int}, collection::Vector{String})\n\nCompute embeddings for the PIDs sampled by _sample_pids.\n\nThe embedding array has shape (D, N), where D is the embedding dimension (128, after applying the linear layer of the ColBERT model) and N is the total number of embeddings over all documents.\n\nArguments\n\nbert: The pre-trained BERT component of ColBERT.\nlinear: The pre-trained linear component of ColBERT.\ntokenizer: The tokenizer to be used.\ndim: The embedding dimension.\nindex_bsize: The batch size to be used to run the transformer. See ColBERTConfig.\ndoc_token: The document token. See ColBERTConfig.\nskiplist: List of tokens to skip.\ncollection: The underlying collection of passages to get the samples from.\n\nReturns\n\nA tuple containing the average document length (i.e number of attended tokens) computed from the sampled documents, and the embedding matrix for the local samples. The matrix has shape (D, N), where D is the embedding dimension (128) and N is the total number of embeddings over all the sampled passages.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._sample_pids-Tuple{Int64}","page":"Reference","title":"ColBERT._sample_pids","text":"_sample_pids(num_documents::Int)\n\nSample PIDs from the collection to be used to compute clusters using a k-means clustering algorithm.\n\nArguments\n\nnum_documents: The total number of documents in the collection. It is assumed that each document has an ID (aka PID) in the range of integers between 1 and num_documents (both inclusive).\n\nReturns\n\nA Set of Ints containing the sampled PIDs.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._unbinarize-Tuple{AbstractArray{Bool, 3}}","page":"Reference","title":"ColBERT._unbinarize","text":"Examples\n\njulia> using ColBERT: _binarize, _unbinarize;\n\njulia> using Flux, CUDA, Random;\n\njulia> Random.seed!(0);\n\njulia> nbits = 5;\n\njulia> data = rand(0:2^nbits - 1, 100, 200000) |> Flux.gpu\n\njulia> binarized_data = _binarize(data, nbits);\n\njulia> unbinarized_data = _unbinarize(binarized_data);\n\njulia> isequal(unbinarized_data, data)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT._unpackbits-Tuple{AbstractMatrix{UInt8}, Int64}","page":"Reference","title":"ColBERT._unpackbits","text":"Examples\n\njulia> using ColBERT: _unpackbits;\n\njulia> using Random; Random.seed!(0);\n\njulia> dim, nbits = 128, 2;\n\njulia> bitsarray = rand(Bool, nbits, dim, 200000);\n\njulia> packedbits = _packbits(bitsarray);\n\njulia> unpackedarray = _unpackbits(packedbits, nbits);\n\njulia> isequal(bitsarray, unpackedarray)\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.binarize-Tuple{Int64, Int64, Vector{Float32}, AbstractMatrix{Float32}}","page":"Reference","title":"ColBERT.binarize","text":"binarize(dim::Int, nbits::Int, bucket_cutoffs::Vector{Float32},\n    residuals::AbstractMatrix{Float32})\n\nConvert a matrix of residual vectors into a matrix of integer residual vector using nbits bits.\n\nArguments\n\ndim: The embedding dimension (see ColBERTConfig).\nnbits: Number of bits to compress the residuals into.\nbucket_cutoffs: Cutoffs used to determine residual buckets.\nresiduals: The matrix of residuals ot be compressed.\n\nReturns\n\nA AbstractMatrix{UInt8} of compressed integer residual vectors.\n\nExamples\n\njulia> using ColBERT: binarize;\n\njulia> using Statistics, Random;\n\njulia> Random.seed!(0);\n\njulia> dim, nbits = 128, 2;           # encode residuals in 2 bits\n\njulia> residuals = rand(Float32, dim, 200000);\n\njulia> quantiles = collect(0:(2^nbits - 1)) / 2^nbits;\n\njulia> bucket_cutoffs = Float32.(quantile(residuals, quantiles[2:end]))\n3-element Vector{Float32}:\n 0.2502231\n 0.5001043\n 0.75005275\n\njulia> binarize(dim, nbits, bucket_cutoffs, residuals)\n32×200000 Matrix{UInt8}:\n 0xb4  0xa2  0x0f  0xd5  0xe2  0xd3  0x03  0xbe  0xe3  …  0x44  0xf5  0x8c  0x62  0x59  0xdc  0xc9  0x9e  0x57\n 0xce  0x7e  0x23  0xd8  0xea  0x96  0x23  0x3e  0xe1     0xfb  0x29  0xa5  0xab  0x28  0xc3  0xed  0x60  0x90\n 0xb1  0x3e  0x96  0xc9  0x84  0x73  0x2c  0x28  0x22     0x27  0x6e  0xca  0x19  0xcd  0x9f  0x1a  0xf4  0xe4\n 0xd8  0x85  0x26  0xe2  0xf8  0xfc  0x59  0xef  0x9a     0x51  0xcf  0x06  0x09  0xec  0x0f  0x96  0x94  0x9d\n 0xa7  0xfe  0xe2  0x9a  0xa1  0x5e  0xb0  0xd3  0x98     0x41  0x64  0x7b  0x0c  0xa6  0x69  0x26  0x35  0x05\n 0x12  0x66  0x0c  0x17  0x05  0xff  0xf2  0x35  0xc0  …  0xa6  0xb7  0xda  0x20  0xb4  0xfe  0x33  0xfc  0xa1\n 0x1b  0xa5  0xbc  0xa0  0xc7  0x1c  0xdc  0x43  0x12     0x38  0x81  0x12  0xb1  0x53  0x52  0x50  0x92  0x41\n 0x5b  0xea  0xbe  0x84  0x81  0xed  0xf5  0x83  0x7d     0x4a  0xc8  0x7f  0x95  0xab  0x34  0xcb  0x35  0x15\n 0xd3  0x0a  0x18  0xc8  0xea  0x34  0x31  0xcc  0x79     0x39  0x3c  0xec  0xe2  0x6a  0xb2  0x59  0x62  0x74\n 0x1b  0x01  0xee  0xe7  0xda  0xa9  0xe4  0xe6  0xc5     0x75  0x10  0xa1  0xe1  0xe5  0x50  0x23  0xfe  0xa3\n 0xe8  0x38  0x28  0x7c  0x9f  0xd5  0xf7  0x69  0x73  …  0x4e  0xbc  0x52  0xa0  0xca  0x8b  0xe9  0xaf  0xae\n 0x2a  0xa2  0x12  0x1c  0x03  0x21  0x6a  0x6e  0xdb     0xa3  0xe3  0x62  0xb9  0x69  0xc0  0x39  0x48  0x9a\n 0x76  0x44  0xce  0xd7  0xf7  0x02  0xbd  0xa1  0x7f     0xee  0x5d  0xea  0x9e  0xbe  0x78  0x51  0xbc  0xa3\n 0xb2  0xe6  0x09  0x33  0x5b  0xd1  0xad  0x1e  0x9e     0x2c  0x36  0x09  0xd3  0x60  0x81  0x0f  0xe0  0x9e\n 0xb8  0x18  0x94  0x0a  0x83  0xd0  0x01  0xe1  0x0f     0x76  0x35  0x6d  0x87  0xfe  0x9e  0x9c  0x69  0xe8\n 0x8c  0x6c  0x24  0xf5  0xa9  0xe2  0xbd  0x21  0x83  …  0x1d  0x77  0x11  0xea  0xc1  0xc8  0x09  0xd7  0x4b\n 0x97  0x23  0x9f  0x7a  0x8a  0xd1  0x34  0xc6  0xe7     0xe2  0xd0  0x46  0xab  0xbe  0xb3  0x92  0xeb  0xd8\n 0x10  0x6f  0xce  0x60  0x17  0x2a  0x4f  0x4a  0xb3     0xde  0x79  0xea  0x28  0xa7  0x08  0x68  0x81  0x9c\n 0xae  0xc9  0xc8  0xbf  0x48  0x33  0xa3  0xca  0x8d     0x78  0x4e  0x0e  0xe2  0xe2  0x23  0x08  0x47  0xe6\n 0x41  0x29  0x8e  0xff  0x66  0xcc  0xd8  0x58  0x59     0x92  0xd8  0xef  0x9c  0x3c  0x51  0xd4  0x65  0x64\n 0xb5  0xc4  0x2d  0x30  0x14  0x54  0xd4  0x79  0x62  …  0xff  0xc1  0xed  0xe4  0x62  0xa4  0x12  0xb7  0x47\n 0xcf  0x9a  0x9a  0xd7  0x6f  0xdf  0xad  0x3a  0xf8     0xe5  0x63  0x85  0x0f  0xaf  0x62  0xab  0x67  0x86\n 0x3e  0xc7  0x92  0x54  0x8d  0xef  0x0b  0xd5  0xbb     0x64  0x5a  0x4d  0x10  0x2e  0x8f  0xd4  0xb0  0x68\n 0x7e  0x56  0x3c  0xb5  0xbd  0x63  0x4b  0xf4  0x8a     0x66  0xc7  0x1a  0x39  0x20  0xa4  0x50  0xac  0xed\n 0x3c  0xbc  0x81  0x67  0xb8  0xaf  0x84  0x38  0x8e     0x6e  0x8f  0x3b  0xaf  0xae  0x03  0x0a  0x53  0x55\n 0x3d  0x45  0x76  0x98  0x7f  0x34  0x7d  0x23  0x29  …  0x24  0x3a  0x6b  0x8a  0xb4  0x3c  0x2d  0xe2  0x3a\n 0xed  0x41  0xe6  0x86  0xf3  0x61  0x12  0xc5  0xde     0xd1  0x26  0x11  0x36  0x57  0x6c  0x35  0x38  0xe2\n 0x11  0x57  0x82  0x9b  0x19  0x1f  0x56  0xd7  0x06     0x1e  0x2b  0xd9  0x76  0xa1  0x68  0x27  0xb1  0xde\n 0x89  0xb3  0xeb  0x86  0xbb  0x57  0xda  0xd3  0x5b     0x0e  0x79  0x4c  0x8c  0x57  0x3d  0xf0  0x98  0xb7\n 0xbf  0xc2  0xac  0xf0  0xed  0x69  0x0e  0x19  0x12     0xfe  0xab  0xcd  0xfc  0x72  0x76  0x5c  0x58  0x8b\n 0xe9  0x7b  0xf6  0x22  0xa0  0x60  0x23  0xc9  0x33  …  0x77  0xc7  0xdf  0x8a  0xb9  0xef  0xe3  0x03  0x8a\n 0x6b  0x26  0x08  0x53  0xc3  0x17  0xc4  0x33  0x2e     0xc6  0xb8  0x1e  0x54  0xcd  0xeb  0xb9  0x5f  0x38\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.compress-Tuple{Matrix{Float32}, Vector{Float32}, Int64, Int64, AbstractMatrix{Float32}}","page":"Reference","title":"ColBERT.compress","text":"compress(centroids::Matrix{Float32}, bucket_cutoffs::Vector{Float32},\n    dim::Int, nbits::Int, embs::AbstractMatrix{Float32})\n\nCompress a matrix of embeddings into a compact representation.\n\nAll embeddings are compressed to their nearest centroid IDs and their quantized residual vectors (where the quantization is done in nbits bits). If emb denotes an embedding and centroid is is nearest centroid, the residual vector is defined to be emb - centroid.\n\nArguments\n\ncentroids: The matrix of centroids.\nbucket_cutoffs: Cutoffs used to determine residual buckets.\ndim: The embedding dimension (see ColBERTConfig).\nnbits: Number of bits to compress the residuals into.\nembs: The input embeddings to be compressed.\n\nReturns\n\nA tuple containing a vector of codes and the compressed residuals matrix.\n\nExamples\n\njulia> using ColBERT: compress;\n\njulia> using Random; Random.seed!(0);\n\njulia> nbits, dim = 2, 128;\n\njulia> embs = rand(Float32, dim, 100000);\n\njulia> centroids = embs[:, randperm(size(embs, 2))[1:10000]];\n\njulia> bucket_cutoffs = Float32.(sort(rand(2^nbits - 1)));\n3-element Vector{Float32}:\n 0.08594067\n 0.0968812\n 0.44113323\n\njulia> @time codes, compressed_residuals = compress(\n    centroids, bucket_cutoffs, dim, nbits, embs);\n  4.277926 seconds (1.57 k allocations: 4.238 GiB, 6.46% gc time)\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.compress_into_codes!-Tuple{AbstractVector{UInt32}, AbstractMatrix{Float32}, AbstractMatrix{Float32}}","page":"Reference","title":"ColBERT.compress_into_codes!","text":"compress_into_codes(\n    centroids::AbstractMatrix{Float32}, embs::AbstractMatrix{Float32})\n\nCompresses a matrix of embeddings into a vector of codes using the given centroids, where the code for each embedding is its nearest centroid ID.\n\nArguments\n\ncentroids: The matrix of centroids.\nembs: The matrix of embeddings to be compressed.\n\nReturns\n\nA Vector{UInt32} of codes, where each code corresponds to the nearest centroid ID for the embedding.\n\nExamples\n\njulia> using ColBERT: compress_into_codes;\n\njulia> using Flux, CUDA, Random;\n\njulia> Random.seed!(0);\n\njulia> centroids = rand(Float32, 128, 500) |> Flux.gpu;\n\njulia> embs = rand(Float32, 128, 10000) |> Flux.gpu;\n\njulia> codes = zeros(UInt32, size(embs, 2)) |> Flux.gpu;\n\njulia> @time compress_into_codes!(codes, centroids, embs);\n  0.003489 seconds (4.51 k allocations: 117.117 KiB)\n\njulia> codes\n10000-element CuArray{UInt32, 1, CUDA.DeviceMemory}:\n 0x00000194\n 0x00000194\n 0x0000000b\n 0x000001d9\n 0x0000011f\n 0x00000098\n 0x0000014e\n 0x00000012\n 0x000000a0\n 0x00000098\n 0x000001a7\n 0x00000098\n 0x000001a7\n 0x00000194\n          ⋮\n 0x00000199\n 0x000001a7\n 0x0000014e\n 0x000001a7\n 0x000001a7\n 0x000001a7\n 0x000000ec\n 0x00000098\n 0x000001d9\n 0x00000098\n 0x000001d9\n 0x000001d9\n 0x00000012\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.decompress-Tuple{Int64, Int64, Matrix{Float32}, Vector{Float32}, Vector{UInt32}, AbstractMatrix{UInt8}}","page":"Reference","title":"ColBERT.decompress","text":"Examples\n\njulia> using ColBERT: compress, decompress;\n\njulia> using Random; Random.seed!(0);\n\njulia> nbits, dim = 2, 128;\n\njulia> embs = rand(Float32, dim, 100000);\n\njulia> centroids = embs[:, randperm(size(embs, 2))[1:10000]];\n\njulia> bucket_cutoffs = Float32.(sort(rand(2^nbits - 1)))\n3-element Vector{Float32}:\n 0.08594067\n 0.0968812\n 0.44113323\n\njulia> bucket_weights = Float32.(sort(rand(2^nbits)));\n4-element Vector{Float32}:\n 0.10379179\n 0.25756857\n 0.27798286\n 0.47973529\n\njulia> @time codes, compressed_residuals = compress(\n    centroids, bucket_cutoffs, dim, nbits, embs);\n  4.277926 seconds (1.57 k allocations: 4.238 GiB, 6.46% gc time)\n\njulia> @time decompressed_embeddings = decompress(\n    dim, nbits, centroids, bucket_weights, codes, compressed_residuals);\n0.237170 seconds (276.40 k allocations: 563.049 MiB, 50.93% compilation time)\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.decompress_residuals-Tuple{Int64, Int64, Vector{Float32}, AbstractMatrix{UInt8}}","page":"Reference","title":"ColBERT.decompress_residuals","text":"Examples\n\njulia> using ColBERT: binarize, decompress_residuals;\n\njulia> using Statistics, Flux, CUDA, Random;\n\njulia> Random.seed!(0);\n\njulia> dim, nbits = 128, 2;           # encode residuals in 5 bits\n\njulia> residuals = rand(Float32, dim, 200000);\n\njulia> quantiles = collect(0:(2^nbits - 1)) / 2^nbits;\n\njulia> bucket_cutoffs = Float32.(quantile(residuals, quantiles[2:end]))\n3-element Vector{Float32}:\n 0.2502231\n 0.5001043\n 0.75005275\n\njulia> bucket_weights = Float32.(quantile(residuals, quantiles .+ 0.5 / 2^nbits))\n4-element Vector{Float32}:\n 0.1250611\n 0.37511465\n 0.62501323\n 0.87501866\n\njulia> binary_residuals = binarize(dim, nbits, bucket_cutoffs, residuals);\n\njulia> decompressed_residuals = decompress_residuals(\n    dim, nbits, bucket_weights, binary_residuals)\n128×200000 Matrix{Float32}:\n 0.125061  0.625013  0.875019  0.375115  0.625013  0.875019  …  0.375115  0.125061  0.375115  0.625013  0.875019\n 0.375115  0.125061  0.875019  0.375115  0.125061  0.125061     0.625013  0.875019  0.625013  0.875019  0.375115\n 0.875019  0.625013  0.125061  0.375115  0.625013  0.375115     0.375115  0.375115  0.125061  0.375115  0.375115\n 0.625013  0.625013  0.125061  0.875019  0.875019  0.875019     0.375115  0.875019  0.875019  0.625013  0.375115\n 0.625013  0.625013  0.875019  0.125061  0.625013  0.625013     0.125061  0.875019  0.375115  0.125061  0.125061\n 0.875019  0.875019  0.125061  0.625013  0.625013  0.375115  …  0.625013  0.125061  0.875019  0.125061  0.125061\n 0.125061  0.875019  0.625013  0.375115  0.625013  0.375115     0.625013  0.125061  0.625013  0.625013  0.375115\n 0.875019  0.375115  0.125061  0.875019  0.875019  0.625013     0.125061  0.875019  0.875019  0.375115  0.625013\n 0.375115  0.625013  0.625013  0.375115  0.125061  0.875019     0.375115  0.875019  0.625013  0.125061  0.125061\n 0.125061  0.875019  0.375115  0.625013  0.375115  0.125061     0.875019  0.875019  0.625013  0.375115  0.375115\n 0.875019  0.875019  0.375115  0.125061  0.125061  0.875019  …  0.125061  0.375115  0.375115  0.875019  0.625013\n 0.625013  0.125061  0.625013  0.875019  0.625013  0.375115     0.875019  0.625013  0.125061  0.875019  0.875019\n 0.125061  0.375115  0.625013  0.625013  0.125061  0.125061     0.125061  0.875019  0.625013  0.125061  0.375115\n 0.625013  0.375115  0.375115  0.125061  0.625013  0.875019     0.875019  0.875019  0.375115  0.375115  0.875019\n 0.375115  0.125061  0.625013  0.625013  0.875019  0.875019     0.625013  0.125061  0.375115  0.375115  0.375115\n 0.875019  0.625013  0.125061  0.875019  0.875019  0.875019  …  0.875019  0.125061  0.625013  0.625013  0.625013\n 0.875019  0.625013  0.625013  0.625013  0.375115  0.625013     0.625013  0.375115  0.625013  0.375115  0.375115\n 0.375115  0.875019  0.125061  0.625013  0.125061  0.875019     0.375115  0.625013  0.375115  0.375115  0.375115\n 0.625013  0.875019  0.625013  0.375115  0.625013  0.375115     0.625013  0.625013  0.625013  0.875019  0.125061\n 0.625013  0.875019  0.875019  0.625013  0.625013  0.375115     0.625013  0.375115  0.125061  0.125061  0.125061\n 0.625013  0.625013  0.125061  0.875019  0.375115  0.875019  …  0.125061  0.625013  0.875019  0.125061  0.375115\n 0.125061  0.375115  0.875019  0.375115  0.375115  0.875019     0.375115  0.875019  0.125061  0.875019  0.125061\n 0.375115  0.625013  0.125061  0.375115  0.125061  0.875019     0.875019  0.875019  0.875019  0.875019  0.625013\n 0.125061  0.375115  0.125061  0.125061  0.125061  0.875019     0.625013  0.875019  0.125061  0.875019  0.625013\n 0.875019  0.375115  0.125061  0.125061  0.875019  0.125061     0.875019  0.625013  0.125061  0.625013  0.375115\n 0.625013  0.375115  0.875019  0.125061  0.375115  0.875019  …  0.125061  0.125061  0.125061  0.125061  0.125061\n 0.375115  0.625013  0.875019  0.625013  0.125061  0.375115     0.375115  0.375115  0.375115  0.375115  0.125061\n ⋮                                                 ⋮         ⋱  ⋮\n 0.875019  0.375115  0.375115  0.625013  0.875019  0.375115     0.375115  0.875019  0.875019  0.125061  0.625013\n 0.875019  0.125061  0.875019  0.375115  0.875019  0.875019     0.875019  0.875019  0.625013  0.625013  0.875019\n 0.125061  0.375115  0.375115  0.625013  0.375115  0.125061     0.625013  0.125061  0.125061  0.875019  0.125061\n 0.375115  0.375115  0.625013  0.625013  0.875019  0.375115     0.875019  0.125061  0.375115  0.125061  0.625013\n 0.875019  0.125061  0.375115  0.375115  0.125061  0.125061  …  0.375115  0.875019  0.375115  0.625013  0.125061\n 0.625013  0.125061  0.625013  0.125061  0.875019  0.625013     0.375115  0.625013  0.875019  0.875019  0.625013\n 0.875019  0.375115  0.875019  0.625013  0.875019  0.375115     0.375115  0.375115  0.125061  0.125061  0.875019\n 0.375115  0.875019  0.625013  0.875019  0.375115  0.875019     0.375115  0.125061  0.875019  0.375115  0.625013\n 0.125061  0.375115  0.125061  0.625013  0.625013  0.875019     0.125061  0.625013  0.375115  0.125061  0.875019\n 0.375115  0.375115  0.125061  0.375115  0.375115  0.375115  …  0.625013  0.625013  0.625013  0.875019  0.375115\n 0.125061  0.375115  0.625013  0.625013  0.125061  0.125061     0.625013  0.375115  0.125061  0.625013  0.875019\n 0.375115  0.875019  0.875019  0.625013  0.875019  0.875019     0.875019  0.375115  0.125061  0.125061  0.875019\n 0.625013  0.125061  0.625013  0.375115  0.625013  0.375115     0.375115  0.875019  0.125061  0.625013  0.375115\n 0.125061  0.875019  0.625013  0.125061  0.875019  0.375115     0.375115  0.875019  0.875019  0.375115  0.875019\n 0.625013  0.625013  0.875019  0.625013  0.625013  0.375115  …  0.375115  0.125061  0.875019  0.625013  0.625013\n 0.875019  0.625013  0.125061  0.125061  0.375115  0.375115     0.625013  0.625013  0.125061  0.125061  0.875019\n 0.875019  0.125061  0.875019  0.125061  0.875019  0.625013     0.125061  0.375115  0.875019  0.625013  0.625013\n 0.875019  0.125061  0.625013  0.875019  0.625013  0.625013     0.875019  0.875019  0.375115  0.375115  0.125061\n 0.625013  0.875019  0.625013  0.875019  0.875019  0.375115     0.375115  0.375115  0.375115  0.375115  0.625013\n 0.375115  0.875019  0.625013  0.625013  0.125061  0.125061  …  0.375115  0.875019  0.875019  0.875019  0.625013\n 0.625013  0.625013  0.375115  0.125061  0.125061  0.125061     0.625013  0.875019  0.125061  0.125061  0.625013\n 0.625013  0.875019  0.875019  0.625013  0.625013  0.625013     0.875019  0.625013  0.625013  0.125061  0.125061\n 0.875019  0.375115  0.875019  0.125061  0.625013  0.375115     0.625013  0.875019  0.875019  0.125061  0.625013\n 0.875019  0.625013  0.125061  0.875019  0.875019  0.875019     0.375115  0.875019  0.375115  0.875019  0.125061\n 0.625013  0.375115  0.625013  0.125061  0.125061  0.375115  …  0.875019  0.625013  0.625013  0.875019  0.625013\n 0.625013  0.625013  0.125061  0.375115  0.125061  0.375115     0.125061  0.625013  0.875019  0.375115  0.875019\n 0.375115  0.125061  0.125061  0.375115  0.875019  0.125061     0.875019  0.875019  0.625013  0.375115  0.125061\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.doc-Tuple{Transformers.HuggingFace.HGFBertModel, Transformers.Layers.Dense, AbstractMatrix{Int32}, AbstractMatrix{Bool}}","page":"Reference","title":"ColBERT.doc","text":"doc(bert::HF.HGFBertModel, linear::Layers.Dense,\n    integer_ids::AbstractMatrix{Int32}, bitmask::AbstractMatrix{Bool})\n\nCompute the hidden state of the BERT and linear layers of ColBERT for documents.\n\nArguments\n\nbert: The pre-trained BERT component of the ColBERT model. \nlinear: The pre-trained linear component of the ColBERT model. \ninteger_ids: An array of token IDs to be fed into the BERT model.\ninteger_mask: An array of corresponding attention masks. Should have the same shape as integer_ids.\n\nReturns\n\nAn array D containing the normalized embeddings for each token in each document. It has shape (D, L, N), where D is the embedding dimension (128 for the linear layer of ColBERT), and (L, N) is the shape of integer_ids, i.e L is the maximum length of any document and N is the total number of documents.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.encode_passages-Tuple{Transformers.HuggingFace.HGFBertModel, Transformers.Layers.Dense, Transformers.TextEncoders.AbstractTransformerTextEncoder, Vector{String}, Int64, Int64, String, Vector{Int64}}","page":"Reference","title":"ColBERT.encode_passages","text":"encode_passages(bert::HF.HGFBertModel, linear::Layers.Dense,\n    tokenizer::TextEncoders.AbstractTransformerTextEncoder,\n    passages::Vector{String}, dim::Int, index_bsize::Int,\n    doc_token::String, skiplist::Vector{Int})\n\nEncode a list of document passages.\n\nThe given passages are run through the underlying BERT model and the linear layer to generate the embeddings, after doing relevant document-specific preprocessing.\n\nArguments\n\nbert: The pre-trained BERT component of the ColBERT model. \nlinear: The pre-trained linear component of the ColBERT model. \ntokenizer: The tokenizer to be used. \npassages: A list of strings representing the passages to be encoded.\ndim: The embedding dimension. \nindex_bsize: The batch size to be used for running the transformer. \ndoc_token: The document token. \nskiplist: A list of tokens to skip. \n\nReturns\n\nA tuple embs, doclens where:\n\nembs::AbstractMatrix{Float32}: The full embedding matrix. Of shape (D, N), where D is the embedding dimension and N is the total number of embeddings across all the passages.\ndoclens::AbstractVector{Int}: A vector of document lengths for each passage, i.e the total number of attended tokens for each document passage.\n\nExamples\n\njulia> using ColBERT: load_hgf_pretrained_local, ColBERTConfig, encode_passages;\n\njulia> using CUDA, Flux, Transformers, TextEncodeBase;\n\njulia> config = ColBERTConfig();\n\njulia> dim = config.dim\n128\n\njulia> index_bsize = 128;                       # this is the batch size to be fed in the transformer\n\njulia> doc_maxlen = config.doc_maxlen\n300\n\njulia> doc_token = config.doc_token_id\n\"[unused1]\"\n\njulia> tokenizer, bert, linear = load_hgf_pretrained_local(\"/home/codetalker7/models/colbertv2.0/\");\n\njulia> process = tokenizer.process;\n\njulia> truncpad_pipe = Pipeline{:token}(\n           TextEncodeBase.trunc_and_pad(doc_maxlen - 1, \"[PAD]\", :tail, :tail),\n           :token);\n\njulia> process = process[1:4] |> truncpad_pipe |> process[6:end];\n\njulia> tokenizer = TextEncoders.BertTextEncoder(\n           tokenizer.tokenizer, tokenizer.vocab, process; startsym = tokenizer.startsym,\n           endsym = tokenizer.endsym, padsym = tokenizer.padsym, trunc = tokenizer.trunc);\n\njulia> bert = bert |> Flux.gpu;\n\njulia> linear = linear |> Flux.gpu;\n\njulia> passages = readlines(\"./downloads/lotte/lifestyle/dev/collection.tsv\")[1:1000];\n\njulia> punctuations_and_padsym = [string.(collect(\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"));\n                                   tokenizer.padsym];\n\njulia> skiplist = [lookup(tokenizer.vocab, sym)\n                    for sym in punctuations_and_padsym];\n\njulia> @time embs, doclens = encode_passages(\n    bert, linear, tokenizer, passages, dim, index_bsize, doc_token, skiplist)      # second run stats\n[ Info: Encoding 1000 passages.\n 25.247094 seconds (29.65 M allocations: 1.189 GiB, 37.26% gc time, 0.00% compilation time)\n(Float32[-0.08001435 -0.10785186 … -0.08651956 -0.12118215; 0.07319974 0.06629379 … 0.0929825 0.13665271; … ; -0.037957724 -0.039623592 … 0.031274226 0.063107446; 0.15484622 0.16779025 … 0.11533891 0.11508792], [279, 117, 251, 105, 133, 170, 181, 115, 190, 132  …  76, 204, 199, 244, 256, 125, 251, 261, 262, 263])\n\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.encode_queries-Tuple{Transformers.HuggingFace.HGFBertModel, Transformers.Layers.Dense, Transformers.TextEncoders.AbstractTransformerTextEncoder, Vector{String}, Int64, Int64, String, Bool, Vector{Int64}}","page":"Reference","title":"ColBERT.encode_queries","text":"encode_queries(bert::HF.HGFBertModel, linear::Layers.Dense,\n    tokenizer::TextEncoders.AbstractTransformerTextEncoder,\n    queries::Vector{String}, dim::Int,\n    index_bsize::Int, query_token::String, attend_to_mask_tokens::Bool,\n    skiplist::Vector{Int})\n\nEncode a list of query passages.\n\nArguments\n\nbert: The pre-trained BERT component of the ColBERT model. \nlinear: The pre-trained linear component of the ColBERT model. \ntokenizer: The tokenizer to be used. \nqueries: A list of strings representing the queries to be encoded.\ndim: The embedding dimension. \nindex_bsize: The batch size to be used for running the transformer. \nquery_token: The query token. \nattend_to_mask_tokens: Whether to attend to \"[MASK]\" tokens. \nskiplist: A list of tokens to skip. \n\nReturns\n\nAn array containing the embeddings for each token in the query.\n\nExamples\n\njulia> using ColBERT: load_hgf_pretrained_local, ColBERTConfig, encode_queries;\n\njulia> using CUDA, Flux, Transformers, TextEncodeBase;\n\njulia> config = ColBERTConfig();\n\njulia> dim = config.dim\n128\n\njulia> index_bsize = 128;                       # this is the batch size to be fed in the transformer\n\njulia> query_maxlen = config.query_maxlen\n300\n\njulia> query_token = config.query_token_id\n\"[unused1]\"\n\njulia> tokenizer, bert, linear = load_hgf_pretrained_local(\"/home/codetalker7/models/colbertv2.0/\");\n\njulia> process = tokenizer.process;\n\njulia> truncpad_pipe = Pipeline{:token}(\n           TextEncodeBase.trunc_or_pad(query_maxlen - 1, \"[PAD]\", :tail, :tail),\n           :token);\n\njulia> process = process[1:4] |> truncpad_pipe |> process[6:end];\n\njulia> tokenizer = TextEncoders.BertTextEncoder(\n           tokenizer.tokenizer, tokenizer.vocab, process; startsym = tokenizer.startsym,\n           endsym = tokenizer.endsym, padsym = tokenizer.padsym, trunc = tokenizer.trunc);\n\njulia> bert = bert |> Flux.gpu;\n\njulia> linear = linear |> Flux.gpu;\n\njulia> skiplist = [lookup(tokenizer.vocab, tokenizer.padsym)]\n1-element Vector{Int64}:\n 1\n\njulia> attend_to_mask_tokens = config.attend_to_mask_tokens\n\njulia> queries = [\n    \"what are white spots on raspberries?\",\n    \"here is another query!\",\n];\n\njulia> @time encode_queries(bert, linear, tokenizer, queries, dim, index_bsize,\n    query_token, attend_to_mask_tokens, skiplist);\n[ Info: Encoding 2 queries.\n  0.029858 seconds (27.58 k allocations: 781.727 KiB, 0.00% compilation time)\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.extract_tokenizer_type-Tuple{AbstractString}","page":"Reference","title":"ColBERT.extract_tokenizer_type","text":"extract_tokenizer_type(tkr_type::AbstractString)\n\nExtract tokenizer type from config.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.index-Tuple{Indexer}","page":"Reference","title":"ColBERT.index","text":"index(indexer::Indexer)\n\nBuild an index given the configuration stored in indexer.\n\nArguments\n\nindexer: An Indexer which is used to build the index on disk.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.index-Tuple{String, Transformers.HuggingFace.HGFBertModel, Transformers.Layers.Dense, Transformers.TextEncoders.AbstractTransformerTextEncoder, Vector{String}, Int64, Int64, String, Vector{Int64}, Int64, Int64, AbstractMatrix{Float32}, AbstractVector{Float32}, Int64}","page":"Reference","title":"ColBERT.index","text":"index(index_path::String, bert::HF.HGFBertModel, linear::Layers.Dense,\n    tokenizer::TextEncoders.AbstractTransformerTextEncoder,\n    collection::Vector{String}, dim::Int, index_bsize::Int,\n    doc_token::String, skiplist::Vector{Int}, num_chunks::Int,\n    chunksize::Int, centroids::AbstractMatrix{Float32},\n    bucket_cutoffs::AbstractVector{Float32}, nbits::Int)\n\nBuild the index using for the collection.\n\nThe documents are processed in batches of size chunksize (see setup). Embeddings and document lengths are computed for each batch (see encode_passages), and they are saved to disk along with relevant metadata (see save_chunk).\n\nArguments\n\nindex_path: Path where the index is to be saved. \nbert: The pre-trained BERT component of the ColBERT model. \nlinear: The pre-trained linear component of the ColBERT model. \ntokenizer: Tokenizer to be used. \ncollection: The collection to index.\ndim: The embedding dimension.\nindex_bsize: The batch size used for running the transformer. \ndoc_token: The document token.\nskiplist: List of tokens to skip. \nnum_chunks: Total number of chunks. \nchunksize: The maximum size of a chunk. \ncentroids: Centroids used to compute the compressed representations. \nbucket_cutoffs: Cutoffs used to compute the residuals. \nnbits: Number of bits to encode the residuals in. \n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.kmeans_gpu_onehot!-Tuple{AbstractMatrix{Float32}, AbstractMatrix{Float32}, Int64}","page":"Reference","title":"ColBERT.kmeans_gpu_onehot!","text":"Examples\n\njulia> using ColBERT, Flux, CUDA, Random;\n\njulia> d, n, k = 100, 2000000, 50000 # dimensions, number of points, number of clusters\n(100, 2000000, 50000)\n\njulia> data = rand(Float32, d, n) |> Flux.gpu;           # around 800MB\n\njulia> centroids = data[:, randperm(n)[1:k]];\n\njulia> point_bsize = 1000;         # adjust according to your GPU/CPU memory\n\njulia> @time assignments = ColBERT.kmeans_gpu_onehot!(\n           data, centroids, k; max_iters = 2, point_bsize = point_bsize)\n[ Info: Iteration 1/2, max delta: 0.6814487\n[ Info: Iteration 2/2, max delta: 0.28856403\n 76.381827 seconds (5.76 M allocations: 606.426 MiB, 4.25% gc time, 0.11% compilation time)\n2000000-element Vector{Int32}:\n 24360\n 10954\n 29993\n 22113\n 19024\n 32192\n 33033\n 32738\n 19901\n  5142\n 23567\n 12686\n 18894\n 23919\n  7325\n 29809\n 27885\n 31122\n  1457\n  9823\n 41315\n 14311\n 21975\n 48753\n 16162\n  7809\n 33018\n 22410\n 26646\n  2607\n 34833\n     ⋮\n 15216\n 26424\n 21939\n  9252\n  5071\n 14570\n 22467\n 37881\n 28239\n  8775\n 31290\n  4625\n  7561\n  7645\n  7277\n 36069\n 49799\n 39307\n 10595\n  7639\n 18879\n 12754\n  1233\n 29389\n 24772\n 47907\n 29380\n  1345\n  4781\n 35313\n 30000\n\njulia> centroids\n100×50000 CuArray{Float32, 2, CUDA.DeviceMemory}:\n 0.573378  0.509291  0.40079   0.614619  0.593501  0.532985  0.79016    0.573517  …  0.544782  0.666605  0.537127  0.490516  0.74021   0.345155  0.613033\n 0.710199  0.301702  0.570302  0.302831  0.378944  0.28444   0.577703   0.327737     0.27379   0.352727  0.413396  0.49565   0.685949  0.534816  0.540361\n 0.379057  0.424286  0.771943  0.411402  0.319783  0.550557  0.64573    0.679135     0.702826  0.846835  0.608924  0.376951  0.431148  0.642033  0.697345\n 0.694464  0.435644  0.422319  0.532234  0.521483  0.627431  0.501389   0.359163     0.328353  0.350925  0.485843  0.437292  0.354213  0.185923  0.427814\n 0.221736  0.506781  0.352585  0.678622  0.333673  0.50622   0.463275   0.591525     0.572961  0.473792  0.369353  0.400138  0.733724  0.477619  0.254028\n 0.619385  0.51777   0.40583   0.445265  0.224872  0.677207  0.713577   0.620289  …  0.389378  0.487728  0.675865  0.250588  0.614895  0.668617  0.235178\n 0.591426  0.395195  0.538931  0.744411  0.533349  0.338823  0.345266   0.327421     0.373282  0.36309   0.681582  0.646208  0.404389  0.251627  0.341416\n 0.583477  0.423426  0.247412  0.446173  0.280856  0.614167  0.533047   0.573224     0.45711   0.445103  0.697702  0.474529  0.616773  0.460811  0.286667\n 0.49608   0.685452  0.424273  0.683325  0.581213  0.684903  0.382428   0.529762     0.734883  0.71177   0.414117  0.417863  0.543535  0.610839  0.488656\n 0.626167  0.540865  0.677231  0.596885  0.378552  0.398865  0.518733   0.497296     0.661245  0.594468  0.288819  0.29435   0.467833  0.722748  0.663824\n 0.619386  0.579229  0.441548  0.386045  0.564118  0.646701  0.632154   0.612795  …  0.617854  0.597241  0.490215  0.308035  0.349091  0.486332  0.32071\n 0.315375  0.457891  0.642345  0.361314  0.410211  0.380876  0.844302   0.496581     0.726295  0.21279   0.555863  0.468077  0.448128  0.497228  0.688524\n 0.302116  0.55576   0.22489   0.50484   0.561481  0.461971  0.605235   0.627733     0.570166  0.536869  0.647504  0.458224  0.27462   0.553473  0.268046\n 0.745733  0.403701  0.468518  0.418122  0.533233  0.579005  0.837422   0.538135     0.704916  0.666066  0.571446  0.500032  0.585166  0.555079  0.39484\n 0.576735  0.590597  0.312162  0.330425  0.45483   0.279067  0.577954   0.539739     0.644922  0.185377  0.681872  0.36546   0.619736  0.755231  0.818024\n 0.548489  0.695465  0.835756  0.478009  0.412736  0.416005  0.118124   0.626901  …  0.313572  0.754964  0.659507  0.677611  0.479118  0.3991    0.622777\n 0.285406  0.381637  0.338189  0.544162  0.477955  0.546904  0.309153   0.439008     0.563208  0.346864  0.448714  0.383776  0.55155   0.3148    0.467101\n 0.823076  0.652229  0.504614  0.400098  0.357104  0.448227  0.24265    0.696984     0.485136  0.637487  0.643558  0.705938  0.632451  0.424837  0.766686\n 0.421668  0.343106  0.530787  0.528398  0.24584   0.699929  0.214073   0.419076     0.331078  0.35033   0.354848  0.46255   0.475431  0.715539  0.688314\n 0.779925  0.724435  0.638462  0.482254  0.521571  0.715278  0.621099   0.556042     0.308391  0.492443  0.36217   0.408848  0.73595   0.540198  0.698907\n 0.356398  0.544033  0.543013  0.462401  0.402219  0.387093  0.323547   0.373834  …  0.645622  0.674534  0.723415  0.353287  0.613711  0.38006   0.554985\n 0.658572  0.401115  0.25994   0.483548  0.52677   0.712259  0.774561   0.438474     0.376936  0.297307  0.455176  0.23899   0.608517  0.76084   0.382525\n 0.525316  0.362833  0.361821  0.383153  0.248305  0.401027  0.554528   0.278677     0.415318  0.512563  0.401782  0.674682  0.666895  0.663432  0.378345\n 0.580109  0.489022  0.255441  0.590038  0.488305  0.51133   0.508364   0.416333     0.262037  0.348079  0.564498  0.360297  0.702012  0.324764  0.249475\n 0.723813  0.548868  0.550225  0.438456  0.455546  0.714484  0.0994013  0.465583     0.590603  0.414145  0.583897  0.41563   0.411714  0.271341  0.440918\n 0.62465   0.664534  0.342419  0.648037  0.719117  0.665314  0.256789   0.325002  …  0.636772  0.235229  0.472394  0.656942  0.414241  0.216398  0.799625\n 0.409948  0.493941  0.522245  0.38117   0.235328  0.310665  0.557497   0.621436     0.413982  0.577326  0.645292  0.225434  0.430032  0.450371  0.375822\n 0.372894  0.635165  0.494829  0.440398  0.380812  0.755357  0.473521   0.487604     0.349699  0.659922  0.626307  0.437899  0.488775  0.404058  0.64511\n 0.288256  0.491838  0.338052  0.466105  0.363578  0.456235  0.425795   0.453427     0.226024  0.429285  0.604995  0.403821  0.33844   0.254136  0.42694\n 0.314443  0.319862  0.56776   0.652814  0.626939  0.234881  0.274685   0.531139     0.270967  0.547521  0.664938  0.451628  0.531532  0.592488  0.525191\n 0.493068  0.306231  0.562287  0.454218  0.199483  0.57302   0.238318   0.567198  …  0.297332  0.460382  0.285109  0.411792  0.356838  0.340022  0.414451\n 0.53873   0.258357  0.402785  0.269083  0.594396  0.505856  0.690911   0.738276     0.737582  0.369145  0.409122  0.336054  0.358317  0.392364  0.561769\n 0.617347  0.639471  0.333155  0.370546  0.526723  0.293309  0.247984   0.660384     0.647745  0.286011  0.681676  0.624425  0.580846  0.402701  0.297121\n 0.496282  0.378267  0.270501  0.475257  0.516464  0.356405  0.175957   0.539904     0.236559  0.58985   0.578107  0.543669  0.563102  0.71473   0.43457\n 0.297402  0.476382  0.426692  0.283131  0.626477  0.220255  0.372191   0.615784     0.374197  0.55345   0.495846  0.331621  0.645283  0.578616  0.389071\n 0.734077  0.371284  0.826699  0.684061  0.272948  0.693993  0.528874   0.304462  …  0.525932  0.395874  0.500069  0.559787  0.460612  0.798967  0.580689\n ⋮                                                 ⋮                              ⋱                      ⋮\n 0.295452  0.589387  0.339522  0.383816  0.63141   0.505792  0.66544    0.479078     0.448193  0.774786  0.607631  0.349403  0.689084  0.619     0.251087\n 0.342872  0.684608  0.66651   0.402659  0.424726  0.591997  0.391954   0.667982  …  0.459421  0.376128  0.301928  0.538294  0.530345  0.458879  0.59855\n 0.449909  0.409996  0.149798  0.576651  0.290799  0.635566  0.437937   0.511792     0.648198  0.661462  0.61996   0.644484  0.636402  0.527594  0.407358\n 0.782475  0.421017  0.69657   0.691838  0.382575  0.805573  0.364693   0.597721     0.652466  0.666937  0.693412  0.490323  0.514455  0.380534  0.427285\n 0.314463  0.420641  0.364206  0.348991  0.59921   0.746625  0.617284   0.697596     0.342617  0.45338   0.363351  0.660113  0.674676  0.376416  0.721194\n 0.402126  0.588711  0.323173  0.388439  0.34814   0.491494  0.545984   0.648734     0.430481  0.378938  0.309212  0.382807  0.632475  0.367792  0.376823\n 0.555737  0.668767  0.490702  0.663971  0.250589  0.445352  0.172075   0.673576  …  0.322794  0.644713  0.394593  0.572583  0.687199  0.662051  0.3559\n 0.793682  0.698499  0.67152   0.46898   0.656144  0.353421  0.803591   0.633019     0.803097  0.640827  0.365467  0.679615  0.642185  0.685466  0.296224\n 0.428538  0.528681  0.438861  0.625715  0.591183  0.629757  0.456717   0.50485      0.405746  0.437458  0.368839  0.446011  0.488281  0.471933  0.514202\n 0.485429  0.738783  0.287516  0.463954  0.188286  0.544762  0.37223    0.58192      0.585194  0.489835  0.506583  0.464377  0.645507  0.804297  0.786932\n 0.29249   0.586557  0.608833  0.663233  0.576919  0.267828  0.308029   0.712437     0.533969  0.421972  0.476979  0.530931  0.47962   0.528001  0.621458\n 0.279038  0.445135  0.177712  0.515837  0.300508  0.281383  0.400402   0.651     …  0.58635   0.443282  0.657886  0.697657  0.552504  0.329047  0.399654\n 0.832609  0.485713  0.600559  0.699044  0.714713  0.606326  0.273329   0.440225     0.623437  0.667127  0.41734   0.767461  0.702767  0.601694  0.506635\n 0.297328  0.287248  0.36852   0.657753  0.698171  0.719895  0.238376   0.638514     0.343874  0.373995  0.511818  0.377467  0.389039  0.522639  0.686664\n 0.301796  0.737757  0.635025  0.666437  0.393605  0.346305  0.547774   0.689093     0.519264  0.361948  0.718109  0.475808  0.573496  0.514178  0.598478\n 0.549563  0.248966  0.364826  0.57668   0.590149  0.533822  0.664503   0.553704     0.284555  0.591084  0.316526  0.660029  0.516786  0.824489  0.689313\n 0.247931  0.238425  0.23728   0.516849  0.732181  0.405793  0.724634   0.5149    …  0.380765  0.696078  0.41157   0.642839  0.384414  0.493493  0.552407\n 0.606629  0.601705  0.319954  0.533014  0.382539  0.410641  0.29247    0.506377     0.615707  0.501867  0.475531  0.405969  0.333115  0.358202  0.502586\n 0.583896  0.619858  0.593031  0.451623  0.58986   0.349512  0.536081   0.298436     0.396871  0.239656  0.406909  0.541055  0.416507  0.547856  0.424243\n 0.691322  0.50077   0.323869  0.500225  0.420282  0.436531  0.703267   0.541637     0.539365  0.725134  0.693945  0.676646  0.556313  0.374397  0.583554\n 0.701328  0.488743  0.35439   0.613276  0.493706  0.399695  0.728355   0.467517     0.261417  0.575774  0.37854   0.490462  0.461564  0.556492  0.424225\n 0.718797  0.550606  0.565344  0.561342  0.355202  0.578364  0.786034   0.562179  …  0.289592  0.183233  0.524043  0.335948  0.333167  0.476679  0.65326\n 0.701058  0.380252  0.444291  0.532477  0.540552  0.696061  0.403728   0.58757      0.520714  0.510013  0.547041  0.564867  0.532286  0.501574  0.595203\n 0.365637  0.531816  0.565021  0.602144  0.548403  0.764079  0.365481   0.613074     0.360902  0.527056  0.375336  0.544605  0.689852  0.837963  0.459323\n 0.288392  0.268179  0.332016  0.689326  0.234238  0.23735   0.756387   0.532537     0.403286  0.471491  0.602447  0.429769  0.293544  0.437438  0.349532\n 0.664517  0.31624   0.59785   0.230114  0.376591  0.773395  0.752942   0.636399     0.326092  0.72005   0.333086  0.339832  0.325618  0.461294  0.524966\n 0.222333  0.305546  0.673752  0.762977  0.307967  0.312146  0.663083   0.58212   …  0.69865   0.643548  0.640484  0.755733  0.496422  0.649607  0.720769\n 0.411979  0.370252  0.237112  0.311196  0.610508  0.447023  0.506591   0.213862     0.721287  0.373431  0.594912  0.621447  0.43674   0.258687  0.560904\n 0.617416  0.641325  0.560164  0.313925  0.490977  0.337085  0.714373   0.506699     0.253813  0.470016  0.584523  0.447376  0.51011   0.270167  0.484992\n 0.623836  0.324357  0.734953  0.790519  0.455406  0.52695   0.403097   0.446101     0.633619  0.403004  0.694153  0.717927  0.47924   0.576069  0.253169\n 0.73859   0.344694  0.183747  0.69547   0.458342  0.481904  0.737565   0.720339     0.447743  0.619669  0.367867  0.34662   0.607812  0.251007  0.509758\n 0.530767  0.332264  0.550998  0.364326  0.722955  0.580428  0.490779   0.426905  …  0.793421  0.713281  0.779156  0.54861   0.674266  0.21644   0.493613\n 0.343766  0.379023  0.630344  0.744247  0.567047  0.377182  0.73119    0.615484     0.761156  0.264631  0.510148  0.481783  0.453394  0.410757  0.335559\n 0.568994  0.332011  0.631839  0.455666  0.631383  0.453398  0.654253   0.276721     0.268318  0.658483  0.523244  0.549092  0.485578  0.342858  0.436086\n 0.686312  0.268361  0.414777  0.437959  0.617892  0.582933  0.649577   0.342277     0.70994   0.435503  0.24157   0.668377  0.412632  0.667489  0.544822\n 0.446142  0.527333  0.160024  0.325712  0.330222  0.368513  0.661516   0.431168     0.44104   0.665175  0.286649  0.534375  0.67307   0.571995  0.3261\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.load_codec-Tuple{String}","page":"Reference","title":"ColBERT.load_codec","text":"load_codec(index_path::String)\n\nLoad compression/decompression information from the index path.\n\nArguments\n\nindex_path: The path of the index.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.load_config-Tuple{String}","page":"Reference","title":"ColBERT.load_config","text":"load_config(index_path::String)\n\nLoad a ColBERTConfig from disk.\n\nArguments\n\nindex_path: The path of the directory where the config resides.\n\nExamples\n\njulia> using ColBERT;\n\njulia> config = ColBERTConfig(\n           use_gpu = true,\n           collection = \"/home/codetalker7/documents\",\n           index_path = \"./local_index\"\n       );\n\njulia> ColBERT.save(config);\n\njulia> ColBERT.load_config(\"./local_index\")\nColBERTConfig(true, 0, 1, \"[unused0]\", \"[unused1]\", \"[Q]\", \"[D]\", \"colbert-ir/colbertv2.0\", \"/home/codetalker7/documents\", 128, 220, true, 32, false, \"./local_index\", 64, 2, 20, 2, 8192)\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.load_hgf_pretrained_local-Tuple{AbstractString}","page":"Reference","title":"ColBERT.load_hgf_pretrained_local","text":"load_hgf_pretrained_local(dir_spec::AbstractString;\n    path_config::Union{Nothing, AbstractString} = nothing,\n    path_tokenizer_config::Union{Nothing, AbstractString} = nothing,\n    path_special_tokens_map::Union{Nothing, AbstractString} = nothing,\n    path_tokenizer::Union{Nothing, AbstractString} = nothing,\n    path_model::Union{Nothing, AbstractString} = nothing,\n    kwargs...\n\n)\n\nLocal model loader. Honors the load_hgf_pretrained interface, where you can request specific files to be loaded, eg, my/dir/to/model:tokenizer or my/dir/to/model:config.\n\nArguments\n\ndir_spec::AbstractString: Directory specification (item specific after the colon is optional), eg, my/dir/to/model or my/dir/to/model:tokenizer.\npath_config::Union{Nothing, AbstractString}: Path to config file.\npath_tokenizer_config::Union{Nothing, AbstractString}: Path to tokenizer config file.\npath_special_tokens_map::Union{Nothing, AbstractString}: Path to special tokens map file.\npath_tokenizer::Union{Nothing, AbstractString}: Path to tokenizer file.\npath_model::Union{Nothing, AbstractString}: Path to model file.\nkwargs...: Additional keyword arguments for _load_model function like mmap, lazy, trainmode.\n\nExamples\n\njulia> using ColBERT, CUDA;\n\njulia> dir_spec = \"/home/codetalker7/models/colbertv2.0/\";\n\njulia> tokenizer, model, linear = load_hgf_pretrained_local(dir_spec);\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.mask_skiplist!-Tuple{AbstractMatrix{Bool}, AbstractMatrix{Int32}, Vector{Int64}}","page":"Reference","title":"ColBERT.mask_skiplist!","text":"mask_skiplist(tokenizer::TextEncoders.AbstractTransformerTextEncoder,\n    integer_ids::AbstractMatrix{Int32}, skiplist::Union{Missing, Vector{Int64}})\n\nCreate a mask for the given integer_ids, based on the provided skiplist. If the skiplist is not missing, then any token IDs in the list will be filtered out along with the padding token. Otherwise, all tokens are included in the mask.\n\nArguments\n\ntokenizer: The underlying tokenizer.\ninteger_ids: An Array of token IDs for the documents.\nskiplist: A list of token IDs to skip in the mask.\n\nReturns\n\nAn array of booleans indicating whether the corresponding token ID is included in the mask or not. The array has the same shape as integer_ids, i.e (L, N), where L is the maximum length of any document in integer_ids and N is the number of documents.\n\nExamples\n\nIn this example, we'll mask out all punctuations as well as the pad symbol of a tokenizer.\n\njulia> using ColBERT: mask_skiplist;\n\njulia> using TextEncodeBase\n\njulia> tokenizer = load_hgf_pretrained_local(\"/home/codetalker7/models/colbertv2.0/:tokenizer\");\n\njulia> punctuations_and_padsym = [string.(collect(\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"));\n                                   tokenizer.padsym];\n\njulia> skiplist = [lookup(tokenizer.vocab, sym)\n                    for sym in punctuations_and_padsym]\n33-element Vector{Int64}:\n 1000\n 1001\n 1002\n 1003\n 1004\n 1005\n 1006\n 1007\n 1008\n 1009\n 1010\n 1011\n 1012\n 1013\n 1014\n 1025\n 1026\n 1027\n 1028\n 1029\n 1030\n 1031\n 1032\n 1033\n 1034\n 1035\n 1036\n 1037\n 1064\n 1065\n 1066\n 1067\n    1\n\njulia>  batch_text = [\n    \"no punctuation text\",\n    \"this, batch,! of text contains puncts! but is larger so that? the other text contains pad symbol;\"\n];\n\njulia> integer_ids, _ = tensorize_docs(\"[unused1]\", tokenizer, batch_text)\n\njulia> integer_ids\n27×2 Matrix{Int32}:\n   102    102\n     3      3\n  2054   2024\n 26137   1011\n  6594  14109\n 14506   1011\n  3794   1000\n   103   1998\n     1   3794\n     1   3398\n     1  26137\n     1  16650\n     1   1000\n     1   2022\n     1   2004\n     1   3470\n     1   2062\n     1   2009\n     1   1030\n     1   1997\n     1   2061\n     1   3794\n     1   3398\n     1  11688\n     1   6455\n     1   1026\n     1    103\n\njulia> decode(tokenizer, integer_ids)\n27×2 Matrix{String}:\n \" [CLS]\"      \" [CLS]\"\n \" [unused1]\"  \" [unused1]\"\n \" no\"         \" this\"\n \" pun\"        \" ,\"\n \"ct\"          \" batch\"\n \"uation\"      \" ,\"\n \" text\"       \" !\"\n \" [SEP]\"      \" of\"\n \" [PAD]\"      \" text\"\n \" [PAD]\"      \" contains\"\n \" [PAD]\"      \" pun\"\n \" [PAD]\"      \"cts\"\n \" [PAD]\"      \" !\"\n \" [PAD]\"      \" but\"\n \" [PAD]\"      \" is\"\n \" [PAD]\"      \" larger\"\n \" [PAD]\"      \" so\"\n \" [PAD]\"      \" that\"\n \" [PAD]\"      \" ?\"\n \" [PAD]\"      \" the\"\n \" [PAD]\"      \" other\"\n \" [PAD]\"      \" text\"\n \" [PAD]\"      \" contains\"\n \" [PAD]\"      \" pad\"\n \" [PAD]\"      \" symbol\"\n \" [PAD]\"      \" ;\"\n \" [PAD]\"      \" [SEP]\"\n\njulia> mask_skiplist(integer_ids, skiplist)\n27×2 BitMatrix:\n 1  1\n 1  1\n 1  1\n 1  0\n 1  1\n 1  0\n 1  0\n 1  1\n 0  1\n 0  1\n 0  1\n 0  1\n 0  0\n 0  1\n 0  1\n 0  1\n 0  1\n 0  1\n 0  0\n 0  1\n 0  1\n 0  1\n 0  1\n 0  1\n 0  1\n 0  0\n 0  1\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.save-Tuple{ColBERTConfig}","page":"Reference","title":"ColBERT.save","text":"save(config::ColBERTConfig)\n\nSave a ColBERTConfig to disk in JSON.\n\nArguments\n\nconfig: The ColBERTConfig to save.\n\nExamples\n\njulia> using ColBERT;\n\njulia> config = ColBERTConfig(\n           use_gpu = true,\n           collection = \"/home/codetalker7/documents\",\n           index_path = \"./local_index\"\n       );\n\njulia> ColBERT.save(config);\n\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.save_chunk-Tuple{String, AbstractVector{UInt32}, AbstractMatrix{UInt8}, Int64, Int64, AbstractVector{Int64}}","page":"Reference","title":"ColBERT.save_chunk","text":"save_chunk(\n    index_path::String, codes::AbstractVector{UInt32}, residuals::AbstractMatrix{UInt8},\n    chunk_idx::Int, passage_offset::Int, doclens::AbstractVector{Int})\n\nSave a single chunk of compressed embeddings and their relevant metadata to disk.\n\nThe codes and compressed residuals for the chunk are saved in files named <chunk_idx>.codes.jld2. and <chunk_idx>.residuals.jld2 respectively. The document lengths are saved in a file named doclens.<chunk_idx>.jld2. Relevant metadata, including number of documents in the chunk, number of embeddings and the passage offsets are saved in a file named <chunk_idx>.metadata.json.\n\nArguments\n\nindex_path: The path of the index. \ncodes: The codes for the chunk.\nresiduals: The compressed residuals for the chunk.\nchunk_idx: The index of the current chunk being saved.\npassage_offset: The index of the first passage in the chunk.\ndoclens: The document lengths vector for the current chunk.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.save_codec-Tuple{String, Matrix{Float32}, Vector{Float32}, Vector{Float32}, Float32}","page":"Reference","title":"ColBERT.save_codec","text":"save_codec(\n    index_path::String, centroids::Matrix{Float32}, bucket_cutoffs::Vector{Float32},\n    bucket_weights::Vector{Float32}, avg_residual::Float32)\n\nSave compression/decompression information from the index path.\n\nArguments\n\nindex_path: The path of the index.\ncentroids: The matrix of centroids of the index.\nbucket_cutoffs: Cutoffs used to determine buckets during residual compression.\nbucket_weights: Weights used to determine the decompressed values during decompression.\navg_residual: The average residual value, computed from the heldout set (see _compute_avg_residuals).\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.setup-Tuple{Vector{String}, Float32, Int64, Union{Missing, Int64}, Int64}","page":"Reference","title":"ColBERT.setup","text":"setup(collection::Vector{String}, avg_doclen_est::Float32,\n    num_clustering_embs::Int, chunksize::Union{Missing, Int}, nranks::Int)\n\nInitialize the index by computing some indexing-specific estimates and the index plan.\n\nThe number of chunks into which the document embeddings will be stored is simply computed using the number of documents and the size of a chunk. The number of clusters to be used for indexing is computed, and is proportional to 16sqrttextEstimated number of embeddings.\n\nArguments\n\ncollection: The collection of documents to index.\navg_doclen_est: The collection of documents to index.\nnum_clustering_embs: The number of embeddings to be used for computing the clusters.\nchunksize: The size of a chunk to be used. Can be Missing.\nnranks: Number of GPUs. Currently this can only be 1.\n\nReturns\n\nA Dict containing the indexing plan.\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.tensorize_docs-Tuple{String, Transformers.TextEncoders.AbstractTransformerTextEncoder, AbstractArray{String}}","page":"Reference","title":"ColBERT.tensorize_docs","text":"tensorize_docs(doc_token_id::String,\n    tokenizer::TextEncoders.AbstractTransformerTextEncoder,\n    batch_text::Vector{String})\n\nConvert a collection of documents to tensors in the ColBERT format.\n\nThis function adds the document marker token at the beginning of each document and then converts the text data into integer IDs and masks using the tokenizer.\n\nArguments\n\nconfig: The ColBERTConfig to be used to fetch the document marker token ID.\ntokenizer: The tokenizer which is used to convert text data into integer IDs.\nbatch_text: A document texts that will be converted into tensors of token IDs.\n\nReturns\n\nA tuple containing the following is returned:\n\ninteger_ids: A Matrix of token IDs of shape (L, N), where L is the length   of the largest document in batch_text, and N is the number of documents in the batch   being considered.\ninteger_mask: A Matrix of attention masks, of the same shape as integer_ids.\n\nExamples\n\njulia> using ColBERT: tensorize_docs, load_hgf_pretrained_local;\n\njulia> using Transformers, Transformers.TextEncoders, TextEncodeBase;\n\njulia> tokenizer = load_hgf_pretrained_local(\"/home/codetalker7/models/colbertv2.0/:tokenizer\")\n\n# configure the tokenizers maxlen and padding/truncation\njulia> doc_maxlen = 20;\n\njulia> process = tokenizer.process\nPipelines:\n  target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n  target[token] := Transformers.TextEncoders.grouping_sentence(target.token)\n  target[(token, segment)] := SequenceTemplate{String}([CLS]:<type=1> Input[1]:<type=1> [SEP]:<type=1> (Input[2]:<type=2> [SEP]:<type=2>)...)(target.token)\n  target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(512))(target.token)\n  target[token] := TextEncodeBase.trunc_and_pad(512, [PAD], tail, tail)(target.token)\n  target[token] := TextEncodeBase.nested2batch(target.token)\n  target[segment] := TextEncodeBase.trunc_and_pad(512, 1, tail, tail)(target.segment)\n  target[segment] := TextEncodeBase.nested2batch(target.segment)\n  target[sequence_mask] := identity(target.attention_mask)\n  target := (target.token, target.segment, target.attention_mask, target.sequence_mask)\n\njulia> truncpad_pipe = Pipeline{:token}(\n           TextEncodeBase.trunc_and_pad(doc_maxlen - 1, \"[PAD]\", :tail, :tail),\n           :token);\n\njulia> process = process[1:4] |> truncpad_pipe |> process[6:end];\n\njulia> tokenizer = TextEncoders.BertTextEncoder(\n           tokenizer.tokenizer, tokenizer.vocab, process; startsym = tokenizer.startsym,\n           endsym = tokenizer.endsym, padsym = tokenizer.padsym, trunc = tokenizer.trunc);\n\njulia> batch_text = [\n    \"hello world\",\n    \"thank you!\",\n    \"a\",\n    \"this is some longer text, so length should be longer\",\n    \"this is an even longer document. this is some longer text, so length should be longer\",\n];\n\njulia> integer_ids, bitmask = tensorize_docs(\n    \"[unused1]\", tokenizer, batch_text)\n(Int32[102 102 … 102 102; 3 3 … 3 3; … ; 1 1 … 1 2023; 1 1 … 1 2937], Bool[1 1 … 1 1; 1 1 … 1 1; … ; 0 0 … 0 1; 0 0 … 0 1])\n\njulia> integer_ids\n20×5 Matrix{Int32}:\n  102   102   102   102   102\n    3     3     3     3     3\n 7593  4068  1038  2024  2024\n 2089  2018   103  2004  2004\n  103  1000     1  2071  2020\n    1   103     1  2937  2131\n    1     1     1  3794  2937\n    1     1     1  1011  6255\n    1     1     1  2062  1013\n    1     1     1  3092  2024\n    1     1     1  2324  2004\n    1     1     1  2023  2071\n    1     1     1  2937  2937\n    1     1     1   103  3794\n    1     1     1     1  1011\n    1     1     1     1  2062\n    1     1     1     1  3092\n    1     1     1     1  2324\n    1     1     1     1  2023\n    1     1     1     1  2937\n\njulia> bitmask\n20×5 Matrix{Bool}:\n 1  1  1  1  1\n 1  1  1  1  1\n 1  1  1  1  1\n 1  1  1  1  1\n 1  1  0  1  1\n 0  1  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  1  1\n 0  0  0  0  1\n 0  0  0  0  1\n 0  0  0  0  1\n 0  0  0  0  1\n 0  0  0  0  1\n 0  0  0  0  1\n\njulia> TextEncoders.decode(tokenizer, integer_ids)\n20×5 Matrix{String}:\n \"[CLS]\"      \"[CLS]\"      \"[CLS]\"      \"[CLS]\"      \"[CLS]\"\n \"[unused1]\"  \"[unused1]\"  \"[unused1]\"  \"[unused1]\"  \"[unused1]\"\n \"hello\"      \"thank\"      \"a\"          \"this\"       \"this\"\n \"world\"      \"you\"        \"[SEP]\"      \"is\"         \"is\"\n \"[SEP]\"      \"!\"          \"[PAD]\"      \"some\"       \"an\"\n \"[PAD]\"      \"[SEP]\"      \"[PAD]\"      \"longer\"     \"even\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"text\"       \"longer\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \",\"          \"document\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"so\"         \".\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"length\"     \"this\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"should\"     \"is\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"be\"         \"some\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"longer\"     \"longer\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"[SEP]\"      \"text\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \",\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"so\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"length\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"should\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"be\"\n \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"[PAD]\"      \"longer\"\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.tensorize_queries-Tuple{String, Bool, Transformers.TextEncoders.AbstractTransformerTextEncoder, Vector{String}}","page":"Reference","title":"ColBERT.tensorize_queries","text":"using TextEncodeBase: tokenize     tensorizequeries(querytoken::String, attendtomasktokens::Bool,         tokenizer::TextEncoders.AbstractTransformerTextEncoder,         batchtext::Vector{String})\n\nConvert a collection of queries to tensors of token IDs and attention masks.\n\nThis function adds the query marker token at the beginning of each query text and then converts the text data into integer IDs and masks using the tokenizer.\n\nArguments\n\nconfig: The ColBERTConfig to be used to figure out the query marker token ID.\ntokenizer: The tokenizer which is used to convert text data into integer IDs.\nbatch_text: A document texts that will be converted into tensors of token IDs.\n\nReturns\n\nA tuple integer_ids, integer_mask containing the token IDs and the attention mask. Each of these two matrices has shape (L, N), where L is the maximum query length specified by the config (see ColBERTConfig), and N is the number of queries in batch_text.\n\nExamples\n\nIn this example, we first fetch the tokenizer from HuggingFace, and then configure the tokenizer to truncate or pad each sequence to the maximum query length specified by the config. Note that, at the time of writing this package, configuring tokenizers in Transformers.jl doesn't have a clean interface; so, we have to manually configure the tokenizer.\n\njulia> using ColBERT: tensorize_queries, load_hgf_pretrained_local;\n\njulia> using Transformers, Transformers.TextEncoders, TextEncodeBase;\n\njulia> tokenizer = load_hgf_pretrained_local(\"/home/codetalker7/models/colbertv2.0/:tokenizer\");\n\n# configure the tokenizers maxlen and padding/truncation\njulia> query_maxlen = 32;\n\njulia> process = tokenizer.process;\n\njulia> truncpad_pipe = Pipeline{:token}(\n    TextEncodeBase.trunc_or_pad(query_maxlen - 1, \"[PAD]\", :tail, :tail),\n    :token);\n\njulia> process = process[1:4] |> truncpad_pipe |> process[6:end];\n\njulia> tokenizer = TextEncoders.BertTextEncoder(\n    tokenizer.tokenizer, tokenizer.vocab, process; startsym = tokenizer.startsym,\n    endsym = tokenizer.endsym, padsym = tokenizer.padsym, trunc = tokenizer.trunc);\n\njulia> batch_text = [\n    \"what are white spots on raspberries?\",\n    \"what do rabbits eat?\",\n    \"this is a really long query. I'm deliberately making this long\"*\n    \"so that you can actually see that this is really truncated at 32 tokens\"*\n    \"and that the other two queries are padded to get 32 tokens.\"*\n    \"this makes this a nice query as an example.\"\n];\n\njulia> integer_ids, bitmask = tensorize_queries(\n    \"[unused0]\", false, tokenizer, batch_text);\n(Int32[102 102 102; 2 2 2; … ; 104 104 8792; 104 104 2095], Bool[1 1 1; 1 1 1; … ; 0 0 1; 0 0 1])\n\njulia> integer_ids\n32×3 Matrix{Int32}:\n   102    102    102\n     2      2      2\n  2055   2055   2024\n  2025   2080   2004\n  2318  20404   1038\n  7517   4522   2429\n  2007   1030   2147\n 20711    103  23033\n  2362    104   1013\n 20969    104   1046\n  1030    104   1006\n   103    104   1050\n   104    104   9970\n   104    104   2438\n   104    104   2024\n   104    104   2147\n   104    104   6500\n   104    104   2009\n   104    104   2018\n   104    104   2065\n   104    104   2942\n   104    104   2157\n   104    104   2009\n   104    104   2024\n   104    104   2004\n   104    104   2429\n   104    104  25450\n   104    104   2013\n   104    104   3591\n   104    104  19205\n   104    104   8792\n   104    104   2095\n\njulia> bitmask \n32×3 Matrix{Bool}:\n 1  1  1\n 1  1  1\n 1  1  1\n 1  1  1\n 1  1  1\n 1  1  1\n 1  1  1\n 1  1  1\n 1  0  1\n 1  0  1\n 1  0  1\n 1  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n 0  0  1\n\njulia> TextEncoders.decode(tokenizer, integer_ids)\n32×3 Matrix{String}:\n \"[CLS]\"      \"[CLS]\"      \"[CLS]\"\n \"[unused0]\"  \"[unused0]\"  \"[unused0]\"\n \"what\"       \"what\"       \"this\"\n \"are\"        \"do\"         \"is\"\n \"white\"      \"rabbits\"    \"a\"\n \"spots\"      \"eat\"        \"really\"\n \"on\"         \"?\"          \"long\"\n \"ras\"        \"[SEP]\"      \"query\"\n \"##p\"        \"[MASK]\"     \".\"\n \"##berries\"  \"[MASK]\"     \"i\"\n \"?\"          \"[MASK]\"     \"'\"\n \"[SEP]\"      \"[MASK]\"     \"m\"\n \"[MASK]\"     \"[MASK]\"     \"deliberately\"\n \"[MASK]\"     \"[MASK]\"     \"making\"\n \"[MASK]\"     \"[MASK]\"     \"this\"\n \"[MASK]\"     \"[MASK]\"     \"long\"\n \"[MASK]\"     \"[MASK]\"     \"##so\"\n \"[MASK]\"     \"[MASK]\"     \"that\"\n \"[MASK]\"     \"[MASK]\"     \"you\"\n \"[MASK]\"     \"[MASK]\"     \"can\"\n \"[MASK]\"     \"[MASK]\"     \"actually\"\n \"[MASK]\"     \"[MASK]\"     \"see\"\n \"[MASK]\"     \"[MASK]\"     \"that\"\n \"[MASK]\"     \"[MASK]\"     \"this\"\n \"[MASK]\"     \"[MASK]\"     \"is\"\n \"[MASK]\"     \"[MASK]\"     \"really\"\n \"[MASK]\"     \"[MASK]\"     \"truncated\"\n \"[MASK]\"     \"[MASK]\"     \"at\"\n \"[MASK]\"     \"[MASK]\"     \"32\"\n \"[MASK]\"     \"[MASK]\"     \"token\"\n \"[MASK]\"     \"[MASK]\"     \"##san\"\n \"[MASK]\"     \"[MASK]\"     \"##d\"\n\n\n\n\n\n","category":"method"},{"location":"api/#ColBERT.train-Tuple{AbstractMatrix{Float32}, AbstractMatrix{Float32}, Int64, Int64, Int64}","page":"Reference","title":"ColBERT.train","text":"train(sample::AbstractMatrix{Float32}, heldout::AbstractMatrix{Float32},\n    num_partitions::Int, nbits::Int, kmeans_niters::Int)\n\nCompute centroids using a k-means clustering algorithn, and store the compression information on disk.\n\nAverage residuals and other compression data is computed via the _compute_avg_residuals. function.\n\nArguments\n\nsample: The matrix of sampled embeddings used to compute clusters.\nheldout: The matrix of sample embeddings used to compute the residual information.\nnum_partitions: The number of clusters to compute.\nnbits: The number of bits used to encode the residuals.\nkmeans_niters: The maximum number of iterations in the k-means algorithm.\n\nReturns\n\nA Dict containing the residual codec, i.e information used to compress/decompress residuals.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = ColBERT","category":"page"},{"location":"#ColBERT:-Efficient,-late-interaction-retrieval-systems-in-Julia!","page":"Home","title":"ColBERT: Efficient, late-interaction retrieval systems in Julia!","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ColBERT.jl is a pure Julia package for the ColBERT information retrieval system123, allowing developers to integrate this powerful neural retrieval algorithm into their own downstream tasks. ColBERT (contextualized late interaction over BERT) has emerged as a state-of-the-art approach for efficient and effective document retrieval, thanks to its ability to leverage contextualized embeddings from pre-trained language models like BERT.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Inspired from the original Python implementation of ColBERT, with ColBERT.jl, you can now bring this capability to your Julia applications, whether you're working on natural language processing tasks, information retrieval systems, or other areas where relevant document retrieval is crucial. Our package provides a simple and intuitive interface for using ColBERT in Julia, making it easy to get started with this powerful algorithm.","category":"page"},{"location":"#Get-Started","page":"Home","title":"Get Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the package, simply clone the repository and dev it:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ] dev .","category":"page"},{"location":"","page":"Home","title":"Home","text":"Consult the README of the GitHub repository for a small example. In this guide, we'll index a collection of 1000 documents.","category":"page"},{"location":"#Dataset-and-preprocessing","page":"Home","title":"Dataset and preprocessing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We'll go through an example of the lifestyle/dev split of the LoTTe dataset. To download the dataset, you can use the examples/lotte.sh script. We'll work with the first 1000 documents of the dataset:","category":"page"},{"location":"","page":"Home","title":"Home","text":"$ cd examples\n$ ./lotte.sh\n$ head -n 1000 downloads/lotte/lifestyle/dev/collection.tsv > 1kcollection.tsv\n$ wc -l 1kcollection.tsv\n1000 1kcollection.txt","category":"page"},{"location":"","page":"Home","title":"Home","text":"The 1kcollection.tsv file has documents in the format pid \\t <document text>, where pid is the unique ID of the document. For now, the package only supports collections which have one document per line. So, we'll simply remove the pid from each document in 1kcollection.tsv, and save the resultant file of documents in 1kcollection.txt. Here's a simple Julia script you can use to do this preprocessing using the CSV.jl package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using CSV\nfile = CSV.File(\"1kcollection.tsv\"; delim = '\\t', header = [:pid, :text],\n        types = Dict(:pid => Int, :text => String), debug = true, quoted = false)\nfor doc in file.text\n    open(\"1kcollection.txt\", \"a\") do io\n        write(io, doc*\"\\n\")\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"We now have our collection of documents to index!","category":"page"},{"location":"#The-ColBERTConfig","page":"Home","title":"The ColBERTConfig","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To start off, make sure you download a ColBERT checkpoint somewhere in your system; for this example, I'll download the colbert-ir/colbertv2.0 checkpoint in $HOME/models:","category":"page"},{"location":"","page":"Home","title":"Home","text":"git lfs install\ncd $HOME/models\ngit clone https://huggingface.co/colbert-ir/colbertv2.0","category":"page"},{"location":"","page":"Home","title":"Home","text":"The next step is to create a configuration object containing details about all parameters used during indexing/searching using ColBERT. All this information is contained in a type called ColBERTConfig. Creating a ColBERTConfig is easy; it has the right defaults for most users, and one can change the settings using simple kwargs. In this example, we'll create a config for the collection 1kcollection.txt we just created, and we'll also use CUDA.jl for GPU support (you can use any GPU backend supported by Flux.jl)!","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia>  using ColBERT, CUDA, Random;\n\njulia>  Random.seed!(0)                                                 # global seed for a reproducible index\n\njulia>  config = ColBERTConfig(\n            use_gpu = true,\n            checkpoint = \"/home/codetalker7/models/colbertv2.0\",        # local path to the colbert checkpoint\n            collection = \"./1kcollection.txt\",                          # local path to the collection\n            doc_maxlen = 300,                                           # max length beyond which docs are truncated\n            index_path = \"./1kcollection_index/\",                       # local directory to save the index in\n            chunksize = 200                                             # number of docs to store in a chunk\n        );","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can read more about a ColBERTConfig from it's docstring.","category":"page"},{"location":"#Building-the-index","page":"Home","title":"Building the index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Building the index is even easier than creating a config; just build an Indexer and call the index function. I used an NVIDIA GeForce RTX 2020 Ti card to build the index:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia>  indexer = Indexer(config);\n\njulia>  @time index(indexer)\n[ Info: # of sampled PIDs = 636\n[ Info: Encoding 636 passages.\n[ Info: avg_doclen_est = 233.25157232704402      length(local_sample) = 636\n[ Info: Creating 4096 clusters.\n[ Info: Estimated 233251.572327044 embeddings.\n[ Info: Saving the index plan to ./1kcollection_index/plan.json.\n[ Info: Saving the config to the indexing path.\n[ Info: Training the clusters.\n[ Info: Iteration 1/20, max delta: 0.26976448\n[ Info: Iteration 2/20, max delta: 0.17742664\n[ Info: Iteration 3/20, max delta: 0.16281573\n[ Info: Iteration 4/20, max delta: 0.120501295\n[ Info: Iteration 5/20, max delta: 0.08808214\n[ Info: Iteration 6/20, max delta: 0.14226294\n[ Info: Iteration 7/20, max delta: 0.07096822\n[ Info: Iteration 8/20, max delta: 0.081315234\n[ Info: Iteration 9/20, max delta: 0.06760075\n[ Info: Iteration 10/20, max delta: 0.07043305\n[ Info: Iteration 11/20, max delta: 0.060436506\n[ Info: Iteration 12/20, max delta: 0.048092205\n[ Info: Iteration 13/20, max delta: 0.052080974\n[ Info: Iteration 14/20, max delta: 0.055756018\n[ Info: Iteration 15/20, max delta: 0.057068985\n[ Info: Iteration 16/20, max delta: 0.05717972\n[ Info: Iteration 17/20, max delta: 0.02952642\n[ Info: Iteration 18/20, max delta: 0.025388952\n[ Info: Iteration 19/20, max delta: 0.034007154\n[ Info: Iteration 20/20, max delta: 0.047712516\n[ Info: Got bucket_cutoffs_quantiles = [0.25, 0.5, 0.75] and bucket_weights_quantiles = [0.125, 0.375, 0.625, 0.875]\n[ Info: Got bucket_cutoffs = Float32[-0.023658333, -9.9312514f-5, 0.023450013] and bucket_weights = Float32[-0.044035435, -0.010775891, 0.010555617, 0.043713447]\n[ Info: avg_residual = 0.031616904\n[ Info: Saving codec to ./1kcollection_index/centroids.jld2, ./1kcollection_index/avg_residual.jld2, ./1kcollection_index/bucket_cutoffs.jld2 and ./1kcollection_index/bucket_weights.jld2.\n[ Info: Building the index.\n[ Info: Loading codec from ./1kcollection_index/centroids.jld2, ./1kcollection_index/avg_residual.jld2, ./1kcollection_index/bucket_cutoffs.jld2 and ./1kcollection_index/bucket_weights.jld2.\n[ Info: Encoding 200 passages.\n[ Info: Saving chunk 1:          200 passages and 36218 embeddings. From passage #1 onward.\n[ Info: Saving compressed codes to ./1kcollection_index/1.codes.jld2 and residuals to ./1kcollection_index/1.residuals.jld2\n[ Info: Saving doclens to ./1kcollection_index/doclens.1.jld2\n[ Info: Saving metadata to ./1kcollection_index/1.metadata.json\n[ Info: Encoding 200 passages.\n[ Info: Saving chunk 2:          200 passages and 45064 embeddings. From passage #201 onward.\n[ Info: Saving compressed codes to ./1kcollection_index/2.codes.jld2 and residuals to ./1kcollection_index/2.residuals.jld2\n[ Info: Saving doclens to ./1kcollection_index/doclens.2.jld2\n[ Info: Saving metadata to ./1kcollection_index/2.metadata.json\n[ Info: Encoding 200 passages.\n[ Info: Saving chunk 3:          200 passages and 50956 embeddings. From passage #401 onward.\n[ Info: Saving compressed codes to ./1kcollection_index/3.codes.jld2 and residuals to ./1kcollection_index/3.residuals.jld2\n[ Info: Saving doclens to ./1kcollection_index/doclens.3.jld2\n[ Info: Saving metadata to ./1kcollection_index/3.metadata.json\n[ Info: Encoding 200 passages.\n[ Info: Saving chunk 4:          200 passages and 49415 embeddings. From passage #601 onward.\n[ Info: Saving compressed codes to ./1kcollection_index/4.codes.jld2 and residuals to ./1kcollection_index/4.residuals.jld2\n[ Info: Saving doclens to ./1kcollection_index/doclens.4.jld2\n[ Info: Saving metadata to ./1kcollection_index/4.metadata.json\n[ Info: Encoding 200 passages.\n[ Info: Saving chunk 5:          200 passages and 52304 embeddings. From passage #801 onward.\n[ Info: Saving compressed codes to ./1kcollection_index/5.codes.jld2 and residuals to ./1kcollection_index/5.residuals.jld2\n[ Info: Saving doclens to ./1kcollection_index/doclens.5.jld2\n[ Info: Saving metadata to ./1kcollection_index/5.metadata.json\n[ Info: Running some final checks.\n[ Info: Checking if all files are saved.\n[ Info: Found all files!\n[ Info: Collecting embedding ID offsets.\n[ Info: Saving the indexing metadata.\n[ Info: Building the centroid to embedding IVF.\n[ Info: Loading codes for each embedding.\n[ Info: Sorting the codes.\n[ Info: Getting unique codes and their counts.\n[ Info: Saving the IVF.\n151.833047 seconds (78.15 M allocations: 28.871 GiB, 41.12% gc time, 0.51% compilation time: <1% of which was recompilation)","category":"page"},{"location":"#Searching","page":"Home","title":"Searching","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Once you've built the index for your collection of docs, it's now time to perform a query search. This involves creating a Searcher from the path of the index:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia>  using ColBERT, CUDA;\n\njulia>  searcher = Searcher(\"1kcollection_index\");","category":"page"},{"location":"","page":"Home","title":"Home","text":"Next, simply feed a query to the search function, and get the top-k best documents for your query:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia>  query = \"what is 1080 fox bait poisoning?\";\n\njulia>  @time pids, scores = search(searcher, query, 10)            # second run statistics\n  0.136773 seconds (1.95 M allocations: 240.648 MiB, 0.00% compilation time)\n([999, 383, 386, 323, 547, 385, 384, 344, 963, 833], Float32[8.754782, 7.6871076, 6.8440857, 6.365711, 6.323611, 6.1222105, 5.92911, 5.708316, 5.597268, 5.4987035])","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can now use these pids to see which documents match the best against your query:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> print(readlines(\"1kcollection.txt\")[pids[1]])\nTl;dr - Yes, it sounds like a possible 1080 fox bait poisoning. Can't be sure though. The traditional fox bait is called 1080. That poisonous bait is still used in a few countries to kill foxes, rabbits, possums and other mammal pests. The toxin in 1080 is Sodium fluoroacetate. Wikipedia is a bit vague on symptoms in animals, but for humans they say: In humans, the symptoms of poisoning normally appear between 30 minutes and three hours after exposure. Initial symptoms typically include nausea, vomiting and abdominal pain; sweating, confusion and agitation follow. In significant poisoning, cardiac abnormalities including tachycardia or bradycardia, hypotension and ECG changes develop. Neurological effects include muscle twitching and seizures... One might safely assume a dog, especially a small Whippet, would show symptoms of poisoning faster than the 30 mins stated for humans. The listed (human) symptoms look like a good fit to what your neighbour reported about your dog. Strychnine is another commonly used poison against mammal pests. It affects the animal's muscles so that contracted muscles can no longer relax. That means the muscles responsible of breathing cease to operate and the animal suffocates to death in less than two hours. This sounds like unlikely case with your dog. One possibility is unintentional pet poisoning by snail/slug baits. These baits are meant to control a population of snails and slugs in a garden. Because the pelletized bait looks a lot like dry food made for dogs it is easily one of the most common causes of unintentional poisoning of dogs. The toxin in these baits is Metaldehyde and a dog may die inside four hours of ingesting these baits, which sounds like too slow to explain what happened to your dog, even though the symptoms of this toxin are somewhat similar to your case. Then again, the malicious use of poisons against neighbourhood dogs can vary a lot. In fact they don't end with just pesticides but also other harmful matter, like medicine made for humans and even razorblades stuck inside a meatball, have been found in baits. It is quite impossible to say what might have caused the death of your dog, at least without autopsy and toxicology tests. The 1080 is just one of the possible explanations. It is best to always use a leash when walking dogs in populated areas and only let dogs free (when allowed by local legislation) in unpopulated parks and forests and suchlike places.","category":"page"}]
}
